{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# TensorFlow Machine Learning with Financial, New York Times and Watson Data on IBM Power 8 systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This solution presents a time series example on IBM Power8 systems.This is based on the original Google Cloud Platform example documented at:\n",
    "https://cloud.google.com/solutions/machine-learning-with-financial-time-series-data\n",
    "\n",
    "This experiment will focus on the 'renewable energy' sector with structured and unstructured data. The original Goole demo has been extended to\n",
    "\n",
    "In this solution, we will:\n",
    "\n",
    "* Obtain 'renewable energy' data from a number of financial markets (indexes and ETFs).\n",
    "* Obtain from the New York Times articles related to renewable energies\n",
    "* Use Watson to process the articles and extract meaningful data.\n",
    "* Munge that data into a usable format and perform exploratory data analysis in order to explore and validate a premise.\n",
    "* Use TensorFlow to build, train and evaluate a number of models for predicting what will happen in financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## What are the differences with the Google demo ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "SUJNLUdvb2dsZS1EZW1vLUVudi5wbmc=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"IBM-Google-Demo-Env.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Where data are coming from ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The following table shows a number of stock market indices from around the globe, their closing times in Eastern Standard Time (EST), and the delay in hours between the close that index and the close of the stock market in New York. In this experiment we will use the financial data and the time difference to build a predicitve model.\n",
    "\n",
    "|Index|Country|Closing Time (EST)|Hours Before US Close|Index|\n",
    "|---|---|---|---|\n",
    "|[Australian Clean Tech Index](http://www.asx.com.au/)|Australia|01:00|15|asx_cti|\n",
    "|[DAX](http://en.boerse-frankfurt.de/)|Germany|11:30|4.5|dax_eusdn|\n",
    "|[FTSE 100](http://www.londonstockexchange.com/home/homepage.htm)|UK|11:30|4.5|ftse_eo100|\n",
    "|[Credit Suisse](https://www.credit-suisse.com/us/en/investment-banking/indices-research-analytics/indices.html)|UK|11:30|4.5|n8wh|\n",
    "|[Nasdaq](http://www.nasdaq.com/)|US|16:00|0|qcln, cels|\n",
    "|[S&P 500](https://www.standardandpoors.com/en_AU/web/guest/home)|US|16:00|0|icln, sp_gtced|\n",
    "|[Equity Uncertainty Index](https://www.quandl.com/data/PUP/EQUITY_MKT_UNCRTAINTY_INDEX-Economic-Policy-Uncertainty-Equity-Market-Uncertainty-Index)|US|16:00|0|dei|\n",
    "|[Reuters, AP, New York Times](http://developer.nytimes.com/)|US|live stream|N/A|N/A|\n",
    "|[Watson Cognitive Services](https://releaseblueprints.ibm.com/display/WDA/Watson+Discovery+Service)|US|NLP Processing|N/A|N/A|\n",
    "\n",
    "We are also going to use two additional set of data to reinforce our model:\n",
    "\n",
    "1) a series of articles from the New York Times related to the 'clean and renewable energies'. We will try to understand if the 'sentiment' analysis has an influence on the variations of the Stock Market Energy indexes.\n",
    "\n",
    "2) the Equity Uncertainty Index' that reflects the 'mood' of the Market will also be used in the model if we find enough correlation with the other data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: notebook in /usr/local/lib/python2.7/dist-packages (from jupyter)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python2.7/dist-packages (from jupyter)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python2.7/dist-packages (from jupyter)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python2.7/dist-packages (from jupyter)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python2.7/dist-packages (from jupyter)\n",
      "Requirement already satisfied: qtconsole in /usr/local/lib/python2.7/dist-packages (from jupyter)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python2.7/dist-packages (from notebook->jupyter)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python2.7/dist-packages (from notebook->jupyter)\n",
      "Requirement already satisfied: tornado>=4 in /usr/local/lib/python2.7/dist-packages (from notebook->jupyter)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python2.7/dist-packages (from notebook->jupyter)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python2.7/dist-packages (from notebook->jupyter)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python2.7/dist-packages (from notebook->jupyter)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python2.7/dist-packages (from notebook->jupyter)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python2.7/dist-packages (from notebook->jupyter)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python2.7/dist-packages (from nbconvert->jupyter)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from nbconvert->jupyter)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python2.7/dist-packages (from nbconvert->jupyter)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python2.7/dist-packages (from nbconvert->jupyter)\n",
      "Requirement already satisfied: mistune!=0.6 in /usr/local/lib/python2.7/dist-packages (from nbconvert->jupyter)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python2.7/dist-packages (from nbconvert->jupyter)\n",
      "Requirement already satisfied: ipython<6.0.0,>=4.0.0; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from ipywidgets->jupyter)\n",
      "Requirement already satisfied: widgetsnbextension~=3.0.0 in /usr/local/lib/python2.7/dist-packages (from ipywidgets->jupyter)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python2.7/dist-packages (from jupyter-console->jupyter)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python2.7/dist-packages (from jinja2->notebook->jupyter)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python2.7/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python2.7/dist-packages (from tornado>=4->notebook->jupyter)\n",
      "Requirement already satisfied: singledispatch in /usr/local/lib/python2.7/dist-packages (from tornado>=4->notebook->jupyter)\n",
      "Requirement already satisfied: backports-abc>=0.4 in /usr/local/lib/python2.7/dist-packages (from tornado>=4->notebook->jupyter)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python2.7/dist-packages (from jupyter-client->notebook->jupyter)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python2.7/dist-packages (from jupyter-client->notebook->jupyter)\n",
      "Requirement already satisfied: six in /opt/DL/tensorflow/lib/python2.7/site-packages (from traitlets>=4.2.1->notebook->jupyter)\n",
      "Requirement already satisfied: enum34; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.2.1->notebook->jupyter)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.2.1->notebook->jupyter)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python2.7/dist-packages (from nbformat->notebook->jupyter)\n",
      "Requirement already satisfied: configparser>=3.5; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from entrypoints>=0.2.2->nbconvert->jupyter)\n",
      "Requirement already satisfied: html5lib>=0.99999999 in /usr/local/lib/python2.7/dist-packages (from bleach->nbconvert->jupyter)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/DL/tensorflow/lib/python2.7/site-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: backports.shutil-get-terminal-size; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: pathlib2; python_version == \"2.7\" or python_version == \"3.3\" in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python2.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter)\n",
      "Requirement already satisfied: functools32; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook->jupyter)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python2.7/dist-packages (from html5lib>=0.99999999->bleach->nbconvert->jupyter)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/DL/tensorflow/lib/python2.7/site-packages (from setuptools>=18.5->ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: appdirs>=1.4.0 in /opt/DL/tensorflow/lib/python2.7/site-packages (from setuptools>=18.5->ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: scandir; python_version < \"3.5\" in /usr/local/lib/python2.7/dist-packages (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: pyparsing in /opt/DL/tensorflow/lib/python2.7/site-packages (from packaging>=16.8->setuptools>=18.5->ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets->jupyter)\n",
      "Requirement already satisfied: ijson in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: pytz>=2011k in /usr/lib/python2.7/dist-packages (from pandas)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/lib/python2.7/dist-packages (from pandas)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas)\n",
      "Requirement already satisfied: six>=1.5 in /opt/DL/tensorflow/lib/python2.7/site-packages (from python-dateutil->pandas)\n",
      "Requirement already satisfied: pandas_datareader in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python2.7/dist-packages (from pandas_datareader)\n",
      "Requirement already satisfied: requests-ftp in /usr/local/lib/python2.7/dist-packages (from pandas_datareader)\n",
      "Requirement already satisfied: requests-file in /usr/local/lib/python2.7/dist-packages (from pandas_datareader)\n",
      "Requirement already satisfied: requests>=2.3.0 in /usr/lib/python2.7/dist-packages (from pandas_datareader)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/lib/python2.7/dist-packages (from pandas>=0.17.0->pandas_datareader)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/lib/python2.7/dist-packages (from pandas>=0.17.0->pandas_datareader)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas>=0.17.0->pandas_datareader)\n",
      "Requirement already satisfied: six in /opt/DL/tensorflow/lib/python2.7/site-packages (from requests-file->pandas_datareader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httplib2 in /usr/local/lib/python2.7/dist-packages\n",
      "Collecting watson_developer_cloud\n",
      "Collecting pyOpenSSL>=16.2.0 (from watson_developer_cloud)\n",
      "  Using cached pyOpenSSL-17.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: requests<3.0,>=2.0 in /usr/lib/python2.7/dist-packages (from watson_developer_cloud)\n",
      "Requirement already satisfied: pysolr<4.0,>=3.3 in /usr/local/lib/python2.7/dist-packages (from watson_developer_cloud)\n",
      "Collecting cryptography>=1.9 (from pyOpenSSL>=16.2.0->watson_developer_cloud)\n",
      "  Using cached cryptography-2.0.3.tar.gz\n",
      "Requirement already satisfied: six>=1.5.2 in /opt/DL/tensorflow/lib/python2.7/site-packages (from pyOpenSSL>=16.2.0->watson_developer_cloud)\n",
      "Requirement already satisfied: idna>=2.1 in /usr/local/lib/python2.7/dist-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson_developer_cloud)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in /usr/local/lib/python2.7/dist-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson_developer_cloud)\n",
      "Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson_developer_cloud)\n",
      "Requirement already satisfied: ipaddress in /usr/lib/python2.7/dist-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson_developer_cloud)\n",
      "Collecting cffi>=1.7 (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson_developer_cloud)\n",
      "  Using cached cffi-1.10.0.tar.gz\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python2.7/dist-packages (from cffi>=1.7->cryptography>=1.9->pyOpenSSL>=16.2.0->watson_developer_cloud)\n",
      "Building wheels for collected packages: cryptography, cffi\n",
      "  Running setup.py bdist_wheel for cryptography ... \u001b[?25lerror\n",
      "  Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-Fr9k5x/cryptography/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmp3us6pQpip-wheel- --python-tag cp27:\n",
      "  c/_cffi_backend.c:15:17: fatal error: ffi.h: No such file or directory\n",
      "  compilation terminated.\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-build-Fr9k5x/cryptography/setup.py\", line 312, in <module>\n",
      "      **keywords_with_side_effects(sys.argv)\n",
      "    File \"/usr/lib/python2.7/distutils/core.py\", line 111, in setup\n",
      "      _setup_distribution = dist = klass(attrs)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/dist.py\", line 317, in __init__\n",
      "      self.fetch_build_eggs(attrs['setup_requires'])\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/dist.py\", line 372, in fetch_build_eggs\n",
      "      replace_conflicting=True,\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 851, in resolve\n",
      "      dist = best[req.key] = env.best_match(req, ws, installer)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1123, in best_match\n",
      "      return self.obtain(req, installer)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1135, in obtain\n",
      "      return installer(requirement)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/dist.py\", line 440, in fetch_build_egg\n",
      "      return cmd.easy_install(req)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 674, in easy_install\n",
      "      return self.install_item(spec, dist.location, tmpdir, deps)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 700, in install_item\n",
      "      dists = self.install_eggs(spec, download, tmpdir)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 881, in install_eggs\n",
      "      return self.build_and_install(setup_script, setup_base)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 1120, in build_and_install\n",
      "      self.run_setup(setup_script, setup_base, args)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 1108, in run_setup\n",
      "      raise DistutilsError(\"Setup script exited with %s\" % (v.args[0],))\n",
      "  distutils.errors.DistutilsError: Setup script exited with error: command 'powerpc64le-linux-gnu-gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for cryptography\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for cryptography\n",
      "  Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-Fr9k5x/cryptography/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" clean --all:\n",
      "  c/_cffi_backend.c:15:17: fatal error: ffi.h: No such file or directory\n",
      "  compilation terminated.\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"/tmp/pip-build-Fr9k5x/cryptography/setup.py\", line 312, in <module>\n",
      "      **keywords_with_side_effects(sys.argv)\n",
      "    File \"/usr/lib/python2.7/distutils/core.py\", line 111, in setup\n",
      "      _setup_distribution = dist = klass(attrs)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/dist.py\", line 317, in __init__\n",
      "      self.fetch_build_eggs(attrs['setup_requires'])\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/dist.py\", line 372, in fetch_build_eggs\n",
      "      replace_conflicting=True,\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 851, in resolve\n",
      "      dist = best[req.key] = env.best_match(req, ws, installer)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1123, in best_match\n",
      "      return self.obtain(req, installer)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/pkg_resources/__init__.py\", line 1135, in obtain\n",
      "      return installer(requirement)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/dist.py\", line 440, in fetch_build_egg\n",
      "      return cmd.easy_install(req)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 674, in easy_install\n",
      "      return self.install_item(spec, dist.location, tmpdir, deps)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 700, in install_item\n",
      "      dists = self.install_eggs(spec, download, tmpdir)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 881, in install_eggs\n",
      "      return self.build_and_install(setup_script, setup_base)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 1120, in build_and_install\n",
      "      self.run_setup(setup_script, setup_base, args)\n",
      "    File \"/opt/DL/tensorflow/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 1108, in run_setup\n",
      "      raise DistutilsError(\"Setup script exited with %s\" % (v.args[0],))\n",
      "  distutils.errors.DistutilsError: Setup script exited with error: command 'powerpc64le-linux-gnu-gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed cleaning build dir for cryptography\u001b[0m\n",
      "  Running setup.py bdist_wheel for cffi ... \u001b[?25lerror\n",
      "  Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-Fr9k5x/cffi/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmpGgLco5pip-wheel- --python-tag cp27:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.linux-ppc64le-2.7\n",
      "  creating build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/verifier.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/backend_ctypes.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/lock.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/setuptools_ext.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/vengine_cpy.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/__init__.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/ffiplatform.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/api.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/cffi_opcode.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/vengine_gen.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/recompiler.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/cparser.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/error.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/model.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/commontypes.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/_cffi_include.h -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/parse_c_type.h -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  copying cffi/_embedding.h -> build/lib.linux-ppc64le-2.7/cffi\n",
      "  running build_ext\n",
      "  building '_cffi_backend' extension\n",
      "  creating build/temp.linux-ppc64le-2.7\n",
      "  creating build/temp.linux-ppc64le-2.7/c\n",
      "  powerpc64le-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/usr/include/python2.7 -c c/_cffi_backend.c -o build/temp.linux-ppc64le-2.7/c/_cffi_backend.o\n",
      "  c/_cffi_backend.c:15:17: fatal error: ffi.h: No such file or directory\n",
      "  compilation terminated.\n",
      "  error: command 'powerpc64le-linux-gnu-gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for cffi\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for cffi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build cryptography cffi\n",
      "Installing collected packages: cffi, cryptography, pyOpenSSL, watson-developer-cloud\n",
      "  Running setup.py install for cffi ... \u001b[?25lerror\n",
      "    Complete output from command /usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-Fr9k5x/cffi/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-FWek7a-record/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.linux-ppc64le-2.7\n",
      "    creating build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/verifier.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/backend_ctypes.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/lock.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/setuptools_ext.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/vengine_cpy.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/__init__.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/ffiplatform.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/api.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/cffi_opcode.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/vengine_gen.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/recompiler.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/cparser.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/error.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/model.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/commontypes.py -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/_cffi_include.h -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/parse_c_type.h -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    copying cffi/_embedding.h -> build/lib.linux-ppc64le-2.7/cffi\n",
      "    running build_ext\n",
      "    building '_cffi_backend' extension\n",
      "    creating build/temp.linux-ppc64le-2.7\n",
      "    creating build/temp.linux-ppc64le-2.7/c\n",
      "    powerpc64le-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -DUSE__THREAD -DHAVE_SYNC_SYNCHRONIZE -I/usr/include/ffi -I/usr/include/libffi -I/usr/include/python2.7 -c c/_cffi_backend.c -o build/temp.linux-ppc64le-2.7/c/_cffi_backend.o\n",
      "    c/_cffi_backend.c:15:17: fatal error: ffi.h: No such file or directory\n",
      "    compilation terminated.\n",
      "    error: command 'powerpc64le-linux-gnu-gcc' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/usr/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-build-Fr9k5x/cffi/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /tmp/pip-FWek7a-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /tmp/pip-build-Fr9k5x/cffi/\u001b[0m\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "build-essential is already the newest version (12.1ubuntu2).\n",
      "build-essential set to manually installed.\n",
      "python-dev is already the newest version (2.7.11-1).\n",
      "python-dev set to manually installed.\n",
      "The following additional packages will be installed:\n",
      "  libssl-doc\n",
      "The following NEW packages will be installed:\n",
      "  libffi-dev libssl-dev libssl-doc\n",
      "0 upgraded, 3 newly installed, 0 to remove and 42 not upgraded.\n",
      "Need to get 2,417 kB of archives.\n",
      "After this operation, 11.1 MB of additional disk space will be used.\n",
      "Do you want to continue? [Y/n] "
     ]
    }
   ],
   "source": [
    "# Nedd to be done only once\n",
    "\n",
    "!pip install jupyter\n",
    "!pip install ijson\n",
    "!pip install pandas\n",
    "\n",
    "!pip install pandas_datareader\n",
    "!pip install httplib2\n",
    "!pip install watson_developer_cloud\n",
    "!apt-get install build-essential libssl-dev libffi-dev python-dev\n",
    "!pip install quandl\n",
    "!apt-get install python-matplotlib\n",
    "!pip install nytimesarticle\n",
    "!apt-get install python-lxml\n",
    "!pip install cython\n",
    "!apt-get install python-scipy\n",
    "!pip install scikit-learn\n",
    "!apt-get install libxml2-dev libxmlsec1-dev\n",
    "!pip install dragnet\n",
    "#### !pip install tensorflow\n",
    "\n",
    "# Install CUDA and CuDNN\n",
    "#\n",
    "# wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2v2_8.0.61-1_ppc64el-deb\n",
    "# dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2v2_8.0.61-1_ppc64el-deb\n",
    "# apt-get update\n",
    "# apt-get install cuda\n",
    "#\n",
    "# Download cuDNN from https://developer.nvidia.com/rdp/cudnn-download  (libcudnn5_5.1.10-1+cuda8.0_ppc64el.deb)\n",
    "# Note: requires a login\n",
    "#\n",
    "# dpkg -i libcudnn5_5.1.10-1+cuda8.0_ppc64el.deb\n",
    "#\n",
    "# Install Tensorflow\n",
    "#\n",
    "# Download the latest mldl-repo-local .deb file from wget https://download.boulder.ibm.com/ibmdl/pub/software/server/mldl/mldl-repo-local_3.3.0_ppc64el.deb\n",
    "# dpkg -i mldl-repo-local*.deb\n",
    "# apt-get update\n",
    "# apt-get install tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, import necessary libraries. All these libraries are installed using 'pip install <package name>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named ijson",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0b55c1434180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mijson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/opt/DL/tensorflow/lib/python2.7/site-packages/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named ijson"
     ]
    }
   ],
   "source": [
    "import StringIO\n",
    "import json\n",
    "import ijson\n",
    "import sys\n",
    "sys.path.insert(0, \"/opt/DL/tensorflow/lib/python2.7/site-packages/\")\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import httplib2\n",
    "import urllib2 \n",
    "from base64 import b64encode\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import time\n",
    "import calendar\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "import time\n",
    "\n",
    "from watson_developer_cloud import AlchemyDataNewsV1\n",
    "import quandl\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from nytimesarticle import articleAPI\n",
    "# api = articleAPI('e232f0c680594b489111a6e7c41cba2c')\n",
    "\n",
    "from dragnet import content_extractor, content_comments_extractor\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Get, merge and unify Stock Market data\n",
    "In this current experiment, the market data are coming from manually retrieved \".csv\" files as some historical data are not available on the public finance websites. If you have a Bloomberg account you can retrieve all the data using the Bloomberg Python API: https://www.bloomberglabs.com/api/libraries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.date(2014, 1, 2)\n",
    "end = datetime.date(2017, 2, 13)\n",
    "# print '{:%Y-%m-%d}'.format(start)\n",
    "# print '{:%Y-%m-%d}'.format(end)\n",
    "\n",
    "# create the index\n",
    "all_days = pd.date_range(start, end, freq='D')\n",
    "\n",
    "# Australian Clean Tech Index (asx_cti)\n",
    "ctius = pd.read_csv('CTIUS.csv')\n",
    "ctius = ctius.set_index(pd.to_datetime(ctius['Date']))\n",
    "\n",
    "# Nasdaq Clean Edge Green Energy Index\n",
    "# cels = pd.read_csv('cels.csv', index_col='Date')\n",
    "cels = pd.read_csv('cels.csv')\n",
    "cels = cels.set_index(pd.to_datetime(cels['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(ctius, cels, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# FTSE, S&P, DAX Clean Energy Index\n",
    "# europe_index = pd.read_csv('DAX-SP-FTSE-index.csv', index_col='Date')\n",
    "europe_index = pd.read_csv('DAX-SP-FTSE-index.csv')\n",
    "europe_index = europe_index.set_index(pd.to_datetime(europe_index['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(data, europe_index, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# drop the extra 'Date' columns created during the merge\n",
    "data.drop('Date_x', axis=1, inplace=True)\n",
    "data.drop('Date_y', axis=1, inplace=True)\n",
    "\n",
    "# Credit Suisse\n",
    "n8wh = pd.read_csv('n8wh.csv')\n",
    "n8wh = n8wh.set_index(pd.to_datetime(n8wh['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(data, n8wh, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Nasdaq Clean Edge\n",
    "qcln = pd.read_csv('qcln.csv')\n",
    "qcln = qcln.set_index(pd.to_datetime(qcln['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(data, qcln, left_index=True, right_index=True, how='outer')\n",
    "data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# iSahres S&P Clean Energy\n",
    "icln = pd.read_csv('icln.csv')\n",
    "icln = icln.set_index(pd.to_datetime(icln['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(data, icln, left_index=True, right_index=True, how='outer')\n",
    "data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Pandas includes very convenient functions for filling gaps in the data.\n",
    "# Stock values during the week end or holidays are not available\n",
    "# so we need to fill the gaps\n",
    "data = data.interpolate(method='linear', axis=0).ffill().bfill()\n",
    "\n",
    "# Equity Uncertainty INDEX (dei)\n",
    "eui = pd.read_csv('eui.csv')\n",
    "eui = eui.set_index(pd.to_datetime(eui['Date']))\n",
    "\n",
    "# Merge the data and do some cleanup\n",
    "data = pd.merge(data, eui, left_index=True, right_index=True, how='outer')\n",
    "data.drop('Date_x', axis=1, inplace=True)\n",
    "data.drop('Date_y', axis=1, inplace=True)\n",
    "data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Rename column\n",
    "data=data.rename(columns = {'Daily Equity Index':'dei'})\n",
    "\n",
    "data.tail(5)\n",
    "# print data.index.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "At this point,we've sourced 3+ years of time series for financial indices, combined the pertinent data into a single data structure, and harmonized the data to have the same number of entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Get New York Times data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2,461 'Alternative and Renewable Energy' articles are going to be retrieved from the New York Times database by the cell below. As it takes time due to NYT API limitations, the process has been ran once and the data have been stored in JSON format in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.date(2014, 1, 2)\n",
    "end = datetime.date(2017, 2, 13)\n",
    "# print '{:%Y-%m-%d}'.format(start)\n",
    "# print '{:%Y-%m-%d}'.format(end)\n",
    "# print 'Start processing ....'\n",
    "\n",
    "if os.path.isfile('nyt_article_results.json'):\n",
    "    print 'Data gathered...'\n",
    "else:\n",
    "    all_articles = []\n",
    "    payload = {'fq': 'subject.contains:(\"Alternative and Renewable Energy\",\"Global Warming\",\"Solar Energy\",\"Wind Power\",\"Greenhouse Gas Emissions\")' ,'begin_date': '20140102' , 'end_date': '20170213'}\n",
    "    r = requests.get('https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=<your key>', params=payload)\n",
    "    data = json.loads(r.text)\n",
    "    # print data\n",
    "    total_hits=data['response']['meta']['hits']\n",
    "    print 'Number of articles to process: '+str(total_hits)\n",
    "    time.sleep(10)\n",
    "    counter = 0\n",
    "    while counter < (total_hits/10+1):\n",
    "      print 'Processing '+str(counter)\n",
    "      r = requests.get('https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=<your key>', params={'fq': 'subject.contains:(\"Alternative and Renewable Energy\",\"Global Warming\",\"Solar Energy\",\"Wind Power\",\"Greenhouse Gas Emissions\")' ,'begin_date': '20140102' , 'end_date': '20170213', 'fl': 'web_url,pub_date'  ,'sort': 'oldest', 'page': counter})\n",
    "      time.sleep(10)\n",
    "      counter = counter + 1\n",
    "      data = json.loads(r.text)\n",
    "      with open('nyt_article_temp.json', 'a') as f:\n",
    "          f.write(json.dumps(data,indent=2))\n",
    "          f.close()  \n",
    "      for a_doc in data['response']['docs']:\n",
    "        url = a_doc['web_url']\n",
    "        publication_date = a_doc['pub_date']\n",
    "        a_articles_dict = {\"URL\": url, \"Publication Date\" : publication_date}\n",
    "        all_articles.append(a_articles_dict)\n",
    "    with open('nyt_article_results.json', 'w') as f:\n",
    "      f.write(json.dumps(all_articles,indent=2))\n",
    "      f.close()\n",
    "\n",
    "nyt = pd.read_json('nyt_article_results.json')       \n",
    "nyt.tail(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Get Sentiment Analysis from Watson "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next step is to get the result of the Watson sentiment analysis for each of the arcticles. The   IBM Watson Natural Language Understanding service takes as input text or url. However the Watson service complains when the article has too many links so we add another step, using the Python Dragnet package to purge the HTML formating and send the content of the link as text to Watson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "def watson_request(content):\n",
    "    data = '{\"text\": \"'+content+'\",\"features\": { \"entities\": { \"sentiment\": true, \"limit\": 1 }}}'\n",
    "    response = requests.post('https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2017-02-27', headers=headers, data=data, auth=('<password>', '<user id>'))\n",
    "    # return json.dumps(response.json(), indent=4)\n",
    "    return response.text\n",
    "\n",
    "if os.path.isfile('watson-results.csv'):\n",
    "    print 'Data processed...'\n",
    "else:\n",
    "    res = json.loads(open('nyt_article_results.json').read())\n",
    "    counter = 0\n",
    "\n",
    "    articles = []\n",
    "    for result in res:\n",
    "         # print 'Processing '+ str(counter)\n",
    "         dic = {}\n",
    "         dic['url'] = result['web_url']\n",
    "         dic['date'] = result['pub_date']\n",
    "         # fetch HTML\n",
    "         url = result['web_url']\n",
    "         # print url\n",
    "    \n",
    "         r = requests.get(url)\n",
    "         # get main article without comments\n",
    "         content = content_extractor.analyze(r.content) \n",
    "         response = watson_request(content)\n",
    "         # print response\n",
    "     \n",
    "         # check if a 'score' field is returned\n",
    "         if \"score\" not in response:    \n",
    "            score = 0\n",
    "         else:   \n",
    "             resp_dict = json.loads(response)['entities']\n",
    "             # print resp_dict\n",
    "             n = json.dumps(resp_dict)  \n",
    "             val = json.loads(n) \n",
    "             # print val\n",
    "             score = val[0]['sentiment']['score']\n",
    "     \n",
    "         if score < 0:   \n",
    "             dic['negative'] = abs(score)\n",
    "             dic['positive'] = 0\n",
    "         else:\n",
    "             dic['positive'] = score\n",
    "             dic['negative'] = 0\n",
    "         articles.append(dic)\n",
    "    \n",
    "         counter = counter + 1\n",
    "         time.sleep(1)\n",
    "    \n",
    "    nyt_df = pd.DataFrame(articles) \n",
    "    nyt_df['index'] = pd.to_datetime(nyt_df['date'])\n",
    "\n",
    "    # set index\n",
    "    nyt_df = nyt_df.set_index(['index'])\n",
    "\n",
    "    nyt_df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    # Save dataframe to file\n",
    "    nyt_df.to_csv('watson-results.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Load the file previously processed by the Watson Service.\n",
    "watson_data = pd.read_csv('watson-results.csv', sep='\\t')\n",
    "\n",
    "temp_data = watson_data\n",
    "\n",
    "# format the dataframe to get the date as the index\n",
    "watson_data['index'] = pd.to_datetime(watson_data['index'])\n",
    "# set index\n",
    "watson_data = watson_data.set_index(['index'])\n",
    "\n",
    "# resample based on the date\n",
    "# watson_data = watson_data.resample('D').mean()\n",
    "watson_data = watson_data.resample('D').mean()\n",
    "# change NaN by zeros\n",
    "watson_data = watson_data.fillna(0)\n",
    "# Replace 0 by .0000001 to avoid divide-by-zero errors\n",
    "watson_data = watson_data.replace(0,.000001)\n",
    "\n",
    "watson_data.to_csv('watson.csv', sep=' ')\n",
    "\n",
    "# merge \n",
    "data = pd.merge(data, watson_data, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# watson_data.head(10) \n",
    "temp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Exploratory Data Analysis (EDA) is foundational to working with machine learning, and any other sort of analysis. EDA is about understanding the assumptions and why we are making those assumptions. \n",
    "First, take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can see that the various indices operate on scales differing by orders of magnitude. It's best to scale the data so that, for example, operations involving multiple indices aren't unduly influenced by a single, massive index.\n",
    "\n",
    "Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# N.B. A super-useful trick-ette is to assign the return value of plot to _ \n",
    "# so that you don't get text printed before the plot itself.\n",
    "\n",
    "_ = pd.concat([data['asx_cti'],\n",
    "  data['sp_gtced'],\n",
    "  data['dax_eusdn'],\n",
    "  data['ftse_eo100'],\n",
    "  data['cels'],\n",
    "  data['n8wh'],\n",
    "  data['qcln'],\n",
    "  data['icln'],\n",
    "  data['dei'],               \n",
    "  data['negative'],\n",
    "  data['positive']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As expected, the structure isn't uniformly visible for the indices. Divide each value in an individual index by the maximum value for that index., and then replot. The maximum value of all indices will be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data['asx_cti_scaled'] = data['asx_cti'] / max(data['asx_cti'])\n",
    "data['sp_gtced_scaled'] = data['sp_gtced'] / max(data['sp_gtced'])\n",
    "data['dax_eusdn_scaled'] = data['dax_eusdn'] / max(data['dax_eusdn'])\n",
    "data['ftse_eo100_scaled'] = data['ftse_eo100'] / max(data['ftse_eo100'])\n",
    "data['cels_scaled'] = data['cels'] / max(data['cels'])\n",
    "data['n8wh_scaled'] = data['n8wh'] / max(data['n8wh'])\n",
    "data['qcln_scaled'] = data['qcln'] / max(data['qcln'])\n",
    "data['icln_scaled'] = data['icln'] / max(data['icln'])\n",
    "data['dei_scaled'] = data['dei'] / max(data['dei'])\n",
    "data['positive_scaled'] = data['positive']\n",
    "data['negative_scaled'] = data['negative']\n",
    "\n",
    "\n",
    "# data['positive_scaled'] = data['positive'] / max(data['positive'])\n",
    "# data['negative_scaled'] = data['negative'] / max(data['negative'])\n",
    "\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([data['asx_cti_scaled'],\n",
    "  data['sp_gtced_scaled'],\n",
    "  data['dax_eusdn_scaled'],\n",
    "  data['ftse_eo100_scaled'],\n",
    "  data['cels_scaled'],\n",
    "  data['n8wh_scaled'],\n",
    "  data['qcln_scaled'],\n",
    "  data['icln_scaled'],\n",
    "  data['dei_scaled'],\n",
    "  data['positive_scaled'],\n",
    "  data['negative_scaled']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can see that, over the three-year period, these indices are correlated. Notice that sudden drops from economic events happened globally to all indices, and they otherwise exhibited general rises. This is an good start, though not the complete story. Next, plot autocorrelations for each of the indices. The autocorrelations determine correlations between current values of the index and lagged values of the same index. The goal is to determine whether the lagged values are reliable indicators of the current values. If they are, then we've identified a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "_ = autocorrelation_plot(data['asx_cti'], label='asx_cti')\n",
    "_ = autocorrelation_plot(data['sp_gtced'], label='sp_gtced')\n",
    "_ = autocorrelation_plot(data['dax_eusdn'], label='dax_eusdn')\n",
    "_ = autocorrelation_plot(data['ftse_eo100'], label='ftse_eo100')\n",
    "_ = autocorrelation_plot(data['cels'], label='cels')\n",
    "_ = autocorrelation_plot(data['n8wh'], label='n8wh')\n",
    "_ = autocorrelation_plot(data['qcln'], label='qcln')\n",
    "_ = autocorrelation_plot(data['icln'], label='icln')\n",
    "_ = autocorrelation_plot(data['dei'], label='dei')\n",
    "_ = autocorrelation_plot(data['positive'], label='positive')\n",
    "_ = autocorrelation_plot(data['negative'], label='negative')\n",
    "\n",
    "_ = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We see strong autocorrelations, positive for around 200 and 400 lagged days, depending of the indices, then going negative. This tells us something we should intuitively know: if an index is rising it tends to carry on rising, and vice-versa. It should be encouraging that what we see here conforms to what we know about financial markets.\n",
    "\n",
    "Next, look at a scatter matrix, showing everything plotted against everything, to see how indices are correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = scatter_matrix(pd.concat([data['asx_cti_scaled'],\n",
    "  data['sp_gtced_scaled'],\n",
    "  data['dax_eusdn_scaled'],\n",
    "  data['ftse_eo100_scaled'],\n",
    "  data['cels_scaled'],\n",
    "  data['n8wh_scaled'],\n",
    "  data['qcln_scaled'],\n",
    "  data['icln_scaled'],\n",
    "  data['dei_scaled'],\n",
    "  data['positive'],                            \n",
    "  data['negative']], axis=1), figsize=(20, 20), diagonal='kde')  # kernel density estimate (KDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see significant correlations, further evidence that the premise is workable and one market can be influenced by another. \n",
    "\n",
    "The actual value of an index is not that useful for modeling. It can be a useful indicator, but to get to the heart of the matter, we need a time series that is stationary in the mean, thus having no trend in the data. There are various ways of doing that, but they all essentially look at the difference between values, rather than the absolute value. In the case of market data, the usual practice is to work with logged returns, calculated as the natural logarithm of the index today divided by the index yesterday:\n",
    "\n",
    "`ln(Vt/Vt-1)`\n",
    "\n",
    "There are more reasons why the log return is preferable to the percent return (for example the log is normally distributed and additive), but they don't matter much for this work. What matters is to get to a stationary time series.\n",
    "\n",
    "Calculate and plot the log returns in a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "log_return_data = pd.DataFrame()\n",
    "\n",
    "log_return_data['asx_cti_log_return'] = np.log(data['asx_cti']/data['asx_cti'].shift())\n",
    "log_return_data['sp_gtced_log_return'] = np.log(data['sp_gtced']/data['sp_gtced'].shift())\n",
    "log_return_data['dax_eusdn_log_return'] = np.log(data['dax_eusdn']/data['dax_eusdn'].shift())\n",
    "log_return_data['ftse_eo100_log_return'] = np.log(data['ftse_eo100']/data['ftse_eo100'].shift())\n",
    "log_return_data['cels_log_return'] = np.log(data['cels']/data['cels'].shift())\n",
    "log_return_data['n8wh_log_return'] = np.log(data['n8wh']/data['n8wh'].shift())\n",
    "log_return_data['qcln_log_return'] = np.log(data['qcln']/data['qcln'].shift())\n",
    "log_return_data['icln_log_return'] = np.log(data['icln']/data['icln'].shift())\n",
    "log_return_data['dei_log_return'] = np.log(data['dei']/data['dei'].shift())\n",
    "log_return_data['positive_log_return'] = np.log(data['positive']/data['positive'].shift())\n",
    "log_return_data['negative_log_return'] = np.log(data['negative']/data['negative'].shift())\n",
    "\n",
    "# log_return_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Looking at the log returns, we see that the mean, min, max are all similar. We could go further and center the series on zero, scale them, and normalize the standard deviation, but there's no need to do that at this point. Let's move forward with plotting the data, and iterate if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([log_return_data['asx_cti_log_return'],\n",
    "  log_return_data['sp_gtced_log_return'],\n",
    "  log_return_data['dax_eusdn_log_return'],\n",
    "  log_return_data['ftse_eo100_log_return'],\n",
    "  log_return_data['cels_log_return']], axis=1).plot(figsize=(20, 15))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([log_return_data['n8wh_log_return'],\n",
    "  log_return_data['qcln_log_return'],\n",
    "  log_return_data['icln_log_return']], axis=1).plot(figsize=(20, 15))\n",
    "#  log_return_data['dei_log_return'],\n",
    "#  log_return_data['positive_log_return'],\n",
    "#  log_return_data['negative_log_return']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can see from the plot that the log returns of our indices are similarly scaled and centered, with no visible trend in the data. It's looking good, so now look at autocorrelations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Look at a scatterplot to see how the log return indices correlate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = scatter_matrix(log_return_data, figsize=(20, 20), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The story with the previous scatter plot for log returns is more subtle and more interesting. The US indices are strongly correlated, as expected. The other indices, less so, which is also expected. But there is structure and signal there. Now let's move forward and start to quantify it so we can start to choose features for our model.\n",
    "\n",
    "First look at how the log returns for the closing value of the Trust NASDAQ Cln Edge GrnEngyETF (QCLN) and/or iShares Global Clean Energy ETF (ICLN) correlate with the closing values of other indices available on the same day. This essentially means to assume the indices that close before the QCLN and ICLN (non-US indices) are available and the others (US indices) are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['qcln_0'] = log_return_data['qcln_log_return']     # same day\n",
    "tmp['asx_0'] = log_return_data['asx_cti_log_return']\n",
    "tmp['ftse_0'] = log_return_data['ftse_eo100_log_return']\n",
    "tmp['dax_0'] = log_return_data['dax_eusdn_log_return']\n",
    "tmp['n8wh_0'] = log_return_data['n8wh_log_return']\n",
    "tmp['icln_1'] = log_return_data['icln_log_return'].shift()  # can't get same day value so use previous day\n",
    "tmp['gtced_1'] = log_return_data['sp_gtced_log_return'].shift()\n",
    "tmp['cels_1'] = log_return_data['cels_log_return'].shift()\n",
    "tmp['dei_1'] = log_return_data['dei_log_return'].shift()\n",
    "tmp['positive_1'] = log_return_data['positive_log_return']\n",
    "tmp['negative_1'] = log_return_data['negative_log_return'].shift(2)\n",
    "\n",
    "tmp.corr().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we are directly working with the premise. We're correlating the close of the Trust NASDAQ Cln Edge GrnEngyETF (QCLN) with signals available before the close of the US markets.  And we can see that the Trust NASDAQ Cln Edge GrnEngyETF (QCLN) close is correlated with the Austalian index at .72 which is a very strong correlation and the European indices at around 0.60 for the FTSE and DAX, which is also a strong correlation. \n",
    "\n",
    "Now look at how the log returns for the NASDAQ Cln Edge GrnEngyETF (QCLN) closing values correlate with index values from the previous day to see if they previous closing is predictive. Following from the premise that financial markets are Markov processes, there should be little or no value in historical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['qcln_0'] = log_return_data['qcln_log_return']   \n",
    "tmp['asx_0'] = log_return_data['asx_cti_log_return'].shift()\n",
    "tmp['ftse_0'] = log_return_data['ftse_eo100_log_return'].shift()\n",
    "tmp['dax_0'] = log_return_data['dax_eusdn_log_return'].shift()\n",
    "tmp['n8wh_0'] = log_return_data['n8wh_log_return'].shift()\n",
    "tmp['icln_1'] = log_return_data['icln_log_return'].shift(2)  \n",
    "tmp['gtced_1'] = log_return_data['sp_gtced_log_return'].shift(2)\n",
    "tmp['cels_1'] = log_return_data['cels_log_return'].shift(2)\n",
    "tmp['dei_1'] = log_return_data['dei_log_return'].shift(2)\n",
    "tmp['positive_1'] = log_return_data['positive_log_return'].shift(1)\n",
    "tmp['negative_1'] = log_return_data['negative_log_return'].shift(1)\n",
    "tmp.corr().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We see little to no correlation in this data, meaning that yesterday's values are no practical help in predicting today's close. Let's go one step further and look at correlations between today and the the day before yesterday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Summing up the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Summing up:\n",
    "\n",
    "* Australia index from the same day is a strong predictor for the Nasdaq Energy Index close.\n",
    "* European indices from the same day were a significant predictor for the Nasdaq Energy Index close.\n",
    "* Indices from previous days were not good predictors for the Nasdaq Energy Index close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## PowerAI / TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "PowerAI is IBMs machine learning framework for companies that use servers based on its Power processors and NVIDIAs NVLink high-speed interconnects that allow for data to pass extremely quickly between the processor and the GPU that does most of the deep learning calculations. PowerAI now supports Googles popular Tensorflow machine learning library.\n",
    "[TensorFlow](https://tensorflow.org) is an open source software library, initiated by Google, for numerical computation using data flow graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Training and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "log_return_data['qcln_log_return_positive'] = 0\n",
    "log_return_data.ix[log_return_data['qcln_log_return'] >= 0, 'qcln_log_return_positive'] = 1\n",
    "log_return_data['qcln_log_return_negative'] = 0\n",
    "log_return_data.ix[log_return_data['qcln_log_return'] < 0, 'qcln_log_return_negative'] = 1\n",
    "\n",
    "training_test_data = pd.DataFrame(\n",
    "  columns=[\n",
    "    'qcln_log_return_positive', 'qcln_log_return_negative',\n",
    "    'qcln_log_return_1', 'qcln_log_return_2', 'qcln_log_return_3',\n",
    "    'icln_log_return_1', 'icln_log_return_2', 'icln_log_return_3',\n",
    "    'sp_gtced_log_return_1', 'sp_gtced_log_return_2', 'sp_gtced_log_return_3',\n",
    "    'cels_log_return_1', 'cels_log_return_2', 'cels_log_return_3',  \n",
    "    'asx_cti_log_return_0', 'asx_cti_log_return_1', 'asx_cti_log_return_2',\n",
    "    'ftse_eo100_log_return_0', 'ftse_eo100_log_return_1', 'ftse_eo100_log_return_2',  \n",
    "    'dax_eusdn_log_return_0', 'dax_eusdn_log_return_1', 'dax_eusdn_log_return_2', \n",
    "    'n8wh_log_return_0', 'n8wh_log_return_1', 'n8wh_log_return_2',\n",
    "    'dei_log_return_1', 'dei_log_return_2', 'dei_log_return_3',\n",
    "    'positive_log_return_0', 'positive_log_return_1', 'positive_log_return_2',\n",
    "    'negative_log_return_0', 'negative_log_return_1', 'negative_log_return_2'])\n",
    "\n",
    "for i in range(4, len(log_return_data)):\n",
    "  qcln_log_return_positive = log_return_data['qcln_log_return_positive'].ix[i]\n",
    "  qcln_log_return_negative = log_return_data['qcln_log_return_negative'].ix[i]\n",
    "  qcln_log_return_1 = log_return_data['qcln_log_return'].ix[i-1]\n",
    "  qcln_log_return_2 = log_return_data['qcln_log_return'].ix[i-2]\n",
    "  qcln_log_return_3 = log_return_data['qcln_log_return'].ix[i-3]\n",
    "  icln_log_return_1 = log_return_data['icln_log_return'].ix[i-1]\n",
    "  icln_log_return_2 = log_return_data['icln_log_return'].ix[i-2]\n",
    "  icln_log_return_3 = log_return_data['icln_log_return'].ix[i-3]\n",
    "  sp_gtced_log_return_1 = log_return_data['sp_gtced_log_return'].ix[i-1]\n",
    "  sp_gtced_log_return_2 = log_return_data['sp_gtced_log_return'].ix[i-2]\n",
    "  sp_gtced_log_return_3 = log_return_data['sp_gtced_log_return'].ix[i-3]\n",
    "  cels_log_return_1 = log_return_data['cels_log_return'].ix[i-1]\n",
    "  cels_log_return_2 = log_return_data['cels_log_return'].ix[i-2]\n",
    "  cels_log_return_3 = log_return_data['cels_log_return'].ix[i-3]\n",
    "  asx_cti_log_return_0 = log_return_data['asx_cti_log_return'].ix[i]\n",
    "  asx_cti_log_return_1 = log_return_data['asx_cti_log_return'].ix[i-1]\n",
    "  asx_cti_log_return_2 = log_return_data['asx_cti_log_return'].ix[i-2]\n",
    "  ftse_eo100_log_return_0 = log_return_data['ftse_eo100_log_return'].ix[i]\n",
    "  ftse_eo100_log_return_1 = log_return_data['ftse_eo100_log_return'].ix[i-1]\n",
    "  ftse_eo100_log_return_2 = log_return_data['ftse_eo100_log_return'].ix[i-2]  \n",
    "  dax_eusdn_log_return_0 = log_return_data['dax_eusdn_log_return'].ix[i]\n",
    "  dax_eusdn_log_return_1 = log_return_data['dax_eusdn_log_return'].ix[i-1]\n",
    "  dax_eusdn_log_return_2 = log_return_data['dax_eusdn_log_return'].ix[i-2] \n",
    "  n8wh_log_return_0 = log_return_data['n8wh_log_return'].ix[i]\n",
    "  n8wh_log_return_1 = log_return_data['n8wh_log_return'].ix[i-1]\n",
    "  n8wh_log_return_2 = log_return_data['n8wh_log_return'].ix[i-2] \n",
    "  dei_log_return_1 = log_return_data['dei_log_return'].ix[i-1]\n",
    "  dei_log_return_2 = log_return_data['dei_log_return'].ix[i-2]\n",
    "  dei_log_return_3 = log_return_data['dei_log_return'].ix[i-3]\n",
    "  positive_log_return_0 = log_return_data['positive_log_return'].ix[i]\n",
    "  positive_log_return_1 = log_return_data['positive_log_return'].ix[i-1]\n",
    "  positive_log_return_2 = log_return_data['positive_log_return'].ix[i-2] \n",
    "  negative_log_return_0 = log_return_data['negative_log_return'].ix[i]\n",
    "  negative_log_return_1 = log_return_data['negative_log_return'].ix[i-1]\n",
    "  negative_log_return_2 = log_return_data['negative_log_return'].ix[i-2]   \n",
    "  training_test_data = training_test_data.append(\n",
    "    {'qcln_log_return_positive':qcln_log_return_positive,\n",
    "    'qcln_log_return_negative':qcln_log_return_negative,\n",
    "    'qcln_log_return_1':qcln_log_return_1,\n",
    "    'qcln_log_return_2':qcln_log_return_2,\n",
    "    'qcln_log_return_3':qcln_log_return_3,\n",
    "    'icln_log_return_1':icln_log_return_1,\n",
    "    'icln_log_return_2':icln_log_return_2,\n",
    "    'icln_log_return_3':icln_log_return_3,\n",
    "    'sp_gtced_log_return_1':sp_gtced_log_return_1,\n",
    "    'sp_gtced_log_return_2':sp_gtced_log_return_2,\n",
    "    'sp_gtced_log_return_3':sp_gtced_log_return_3,\n",
    "    'cels_log_return_1':cels_log_return_1,\n",
    "    'cels_log_return_2':cels_log_return_2,\n",
    "    'cels_log_return_3':cels_log_return_3,\n",
    "    'asx_cti_log_return_0':asx_cti_log_return_0,\n",
    "    'asx_cti_log_return_1':asx_cti_log_return_1,\n",
    "    'asx_cti_log_return_2':asx_cti_log_return_2,\n",
    "    'ftse_eo100_log_return_0':ftse_eo100_log_return_0,\n",
    "    'ftse_eo100_log_return_1':ftse_eo100_log_return_1,\n",
    "    'ftse_eo100_log_return_2':ftse_eo100_log_return_2,\n",
    "    'dax_eusdn_log_return_0':dax_eusdn_log_return_0,\n",
    "    'dax_eusdn_log_return_1':dax_eusdn_log_return_1,\n",
    "    'dax_eusdn_log_return_2':dax_eusdn_log_return_2,\n",
    "    'n8wh_log_return_0':n8wh_log_return_0,\n",
    "    'n8wh_log_return_1':n8wh_log_return_1,\n",
    "    'n8wh_log_return_2':n8wh_log_return_2,\n",
    "    'dei_log_return_1':dei_log_return_1,\n",
    "    'dei_log_return_2':dei_log_return_2,\n",
    "    'dei_log_return_3':dei_log_return_3,\n",
    "    'positive_log_return_0':positive_log_return_0,\n",
    "    'positive_log_return_1':positive_log_return_1,\n",
    "    'positive_log_return_2':positive_log_return_2,\n",
    "    'negative_log_return_0':negative_log_return_0,\n",
    "    'negative_log_return_1':negative_log_return_1,\n",
    "    'negative_log_return_2':negative_log_return_2},\n",
    "    ignore_index=True)\n",
    "  \n",
    "# training_test_data.describe()\n",
    "training_test_data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, create the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "predictors_tf = training_test_data[training_test_data.columns[2:]]\n",
    "\n",
    "classes_tf = training_test_data[training_test_data.columns[:2]]\n",
    "\n",
    "training_set_size = int(len(training_test_data) * 0.8)\n",
    "test_set_size = len(training_test_data) - training_set_size\n",
    "\n",
    "training_predictors_tf = predictors_tf[:training_set_size]\n",
    "training_classes_tf = classes_tf[:training_set_size]\n",
    "test_predictors_tf = predictors_tf[training_set_size:]\n",
    "test_classes_tf = classes_tf[training_set_size:]\n",
    "\n",
    "# training_predictors_tf.describe()\n",
    "training_predictors_tf.head(3)\n",
    "# training_classes_tf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Define some metrics here to evaluate the models.\n",
    "\n",
    "* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) -  The ability of the classifier not to label as positive a sample that is negative.\n",
    "* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) - The ability of the classifier to find all the positive samples.\n",
    "* [F1 Score](https://en.wikipedia.org/wiki/F1_score) - A weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "* Accuracy - The percentage correctly predicted in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def tf_confusion_metrics(model, actual_classes, session, feed_dict):\n",
    "  predictions = tf.argmax(model, 1)\n",
    "  actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tp, tn, fp, fn = \\\n",
    "    session.run(\n",
    "      [tp_op, tn_op, fp_op, fn_op], \n",
    "      feed_dict\n",
    "    )\n",
    "\n",
    "  tpr = float(tp)/(float(tp) + float(fn))\n",
    "  fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "  accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "  recall = tpr\n",
    "  precision = float(tp)/(float(tp) + float(fp))\n",
    "  \n",
    "  f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "  \n",
    "  print 'Precision = ', precision\n",
    "  print 'Recall = ', recall\n",
    "  print 'F1 Score = ', f1_score\n",
    "  print 'Accuracy = ', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Binary classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, get some tensors flowing. The model is binary classification expressed in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"binary-classification.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# sess = tf.Session()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "  # Define variables for the number of predictors and number of classes \n",
    "  num_predictors = len(training_predictors_tf.columns) # 24 in the default case\n",
    "  num_classes = len(training_classes_tf.columns) # 2 in the default case\n",
    "\n",
    "# Define placeholders for the data we feed into the process - feature data and actual classes.\n",
    "feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "actual_classes = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights = tf.Variable(tf.truncated_normal([num_predictors, num_classes], stddev=0.0001))\n",
    "biases = tf.Variable(tf.ones([num_classes]))\n",
    "\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our feature data and weights.\n",
    "model = tf.nn.softmax(tf.matmul(feature_data, weights) + biases)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "# Define a training step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "training_step = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(cost)   #0.0001\n",
    "\n",
    "# sess = tf.Session()\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# init = tf.initialize_all_variables()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll train the model over 30,000 iterations using the full dataset each time. Every thousandth iteration we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "def run_it():\n",
    "    for i in range(1, 30001):\n",
    "      sess.run(\n",
    "        training_step, \n",
    "        feed_dict={\n",
    "          feature_data: training_predictors_tf.values, \n",
    "          actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "        }\n",
    "      )\n",
    "      if i%5000 == 0:\n",
    "        print i, sess.run(\n",
    "          accuracy,\n",
    "          feed_dict={\n",
    "            feature_data: training_predictors_tf.values, \n",
    "            actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "          }\n",
    "        )\n",
    "\n",
    "start = time.time()       \n",
    "run_it()\n",
    "end = time.time()\n",
    "print 'Training Time = ', (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Accuracy on the training set is ~69%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feed_dict= {\n",
    "  feature_data: test_predictors_tf.values,\n",
    "  actual_classes: test_classes_tf.values.reshape(len(test_classes_tf.values), 2)\n",
    "}\n",
    "\n",
    "tf_confusion_metrics(model, actual_classes, sess, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Accuracy on the test set is ~60%.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Feed-forward neural network with two hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll now build a proper feed-forward neural net with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"feed-forward.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess1 = tf.Session()\n",
    "\n",
    "nl1 = 50\n",
    "nl2 = 25\n",
    "\n",
    "# Define variables for the number of predictors and number of classes \n",
    "num_predictors = len(training_predictors_tf.columns) # 33 \n",
    "num_classes = len(training_classes_tf.columns) # 2 \n",
    "\n",
    "# Define placeholders for the data we feed into the process - feature data and actual classes.\n",
    "feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "actual_classes = tf.placeholder(\"float\", [None, 2])\n",
    "\n",
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights1 = tf.Variable(tf.truncated_normal([num_predictors,nl1], stddev=0.0001))  #33, 50\n",
    "biases1 = tf.Variable(tf.ones([nl1]))  # 50\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([nl1, nl2], stddev=0.0001))  # 50, 25\n",
    "biases2 = tf.Variable(tf.ones([nl2]))  # 25\n",
    "                     \n",
    "weights3 = tf.Variable(tf.truncated_normal([nl2, 2], stddev=0.0001))   # 25, 2\n",
    "biases3 = tf.Variable(tf.ones([2]))   # 2\n",
    "\n",
    "hidden_layer_1 = tf.nn.relu(tf.matmul(feature_data, weights1) + biases1)   # rectified linear unit (ReLU).\n",
    "hidden_layer_2 = tf.nn.relu(tf.matmul(hidden_layer_1, weights2) + biases2)\n",
    "\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our feature data and weights.\n",
    "model = tf.nn.softmax(tf.matmul(hidden_layer_2, weights3) + biases3)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "# Define a training step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "train_op1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "# init = tf.initialize_all_variables()\n",
    "init = tf.global_variables_initializer()\n",
    "sess1.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll train the model over 15,000 iterations using the full dataset each time. Every thousandth iteration, we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "for i in range(1, 15001):  \n",
    "  sess1.run(\n",
    "    train_op1, \n",
    "    feed_dict={\n",
    "      feature_data: training_predictors_tf.values, \n",
    "      actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%5000 == 0:\n",
    "    print i, sess1.run(\n",
    "      accuracy,\n",
    "      feed_dict={\n",
    "        feature_data: training_predictors_tf.values, \n",
    "        actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "A significant improvement in accuracy with the training data shows that the hidden layers are adding additional capacity for learning to the model.\n",
    "Looking at precision, recall, and accuracy, we can see improvement in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feed_dict= {\n",
    "  feature_data: test_predictors_tf.values,\n",
    "  actual_classes: test_classes_tf.values.reshape(len(test_classes_tf.values), 2)\n",
    "}\n",
    "\n",
    "tf_confusion_metrics(model, actual_classes, sess1, feed_dict)\n",
    "sess1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"NN-Results.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As anticipated, there is a clear dependency between the data and the Neural Network model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Over ~75% accuracy in predicting the close of the Nasdaq Clean Energy Index is the highest we've seen achieved on this dataset. The reason for the relatively modest accuracy achieved is the dataset itself; there isn't enough signal in the current dataset but ~7 times out of 10, we were able to correctly determine if the Nasdaq Clean Energy Index would close up or down on the day, and that's objectively good."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
