{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# TensorFlow Machine Learning with Financial, New York Times and Watson Data on IBM Power 8 systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "This solution presents a time series example on IBM Power8 systems.This is based on the original Google Cloud Platform example documented at:\n",
    "https://cloud.google.com/solutions/machine-learning-with-financial-time-series-data\n",
    "\n",
    "This experiment will focus on the 'renewable energy' sector with structured and unstructured data. The original Goole demo has been extended to\n",
    "\n",
    "In this solution, we will:\n",
    "\n",
    "* Obtain 'renewable energy' data from a number of financial markets (indexes and ETFs).\n",
    "* Obtain from the New York Times articles related to renewable energies\n",
    "* Use Watson to process the articles and extract meaningful data.\n",
    "* Munge that data into a usable format and perform exploratory data analysis in order to explore and validate a premise.\n",
    "* Use TensorFlow to build, train and evaluate a number of models for predicting what will happen in financial markets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## What are the differences with the Google demo ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "SUJNLUdvb2dsZS1EZW1vLUVudi5wbmc=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"IBM-Google-Demo-Env.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Where data are coming from ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The following table shows a number of stock market indices from around the globe, their closing times in Eastern Standard Time (EST), and the delay in hours between the close that index and the close of the stock market in New York. In this experiment we will use the financial data and the time difference to build a predicitve model.\n",
    "\n",
    "|Index|Country|Closing Time (EST)|Hours Before US Close|Index|\n",
    "|---|---|---|---|\n",
    "|[Australian Clean Tech Index](http://www.asx.com.au/)|Australia|01:00|15|asx_cti|\n",
    "|[DAX](http://en.boerse-frankfurt.de/)|Germany|11:30|4.5|dax_eusdn|\n",
    "|[FTSE 100](http://www.londonstockexchange.com/home/homepage.htm)|UK|11:30|4.5|ftse_eo100|\n",
    "|[Credit Suisse](https://www.credit-suisse.com/us/en/investment-banking/indices-research-analytics/indices.html)|UK|11:30|4.5|n8wh|\n",
    "|[Nasdaq](http://www.nasdaq.com/)|US|16:00|0|qcln, cels|\n",
    "|[S&P 500](https://www.standardandpoors.com/en_AU/web/guest/home)|US|16:00|0|icln, sp_gtced|\n",
    "|[Equity Uncertainty Index](https://www.quandl.com/data/PUP/EQUITY_MKT_UNCRTAINTY_INDEX-Economic-Policy-Uncertainty-Equity-Market-Uncertainty-Index)|US|16:00|0|dei|\n",
    "|[Reuters, AP, New York Times](http://developer.nytimes.com/)|US|live stream|N/A|N/A|\n",
    "|[Watson Cognitive Services](https://releaseblueprints.ibm.com/display/WDA/Watson+Discovery+Service)|US|NLP Processing|N/A|N/A|\n",
    "\n",
    "We are also going to use two additional set of data to reinforce our model:\n",
    "\n",
    "1) a series of articles from the New York Times related to the 'clean and renewable energies'. We will try to understand if the 'sentiment' analysis has an influence on the variations of the Stock Market Energy indexes.\n",
    "\n",
    "2) the Equity Uncertainty Index' that reflects the 'mood' of the Market will also be used in the model if we find enough correlation with the other data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dragnet\n",
      "  Using cached dragnet-1.1.0.tar.gz\n",
      "Requirement already satisfied: Cython>=0.21.1 in /Users/Saeed/anaconda/lib/python2.7/site-packages (from dragnet)\n",
      "Requirement already satisfied: lxml in /Users/Saeed/anaconda/lib/python2.7/site-packages (from dragnet)\n",
      "Requirement already satisfied: scikit-learn<0.19.0,>=0.15.2 in /Users/Saeed/anaconda/lib/python2.7/site-packages (from dragnet)\n",
      "Requirement already satisfied: numpy in /Users/Saeed/anaconda/lib/python2.7/site-packages (from dragnet)\n",
      "Requirement already satisfied: scipy in /Users/Saeed/anaconda/lib/python2.7/site-packages (from dragnet)\n",
      "Requirement already satisfied: mozsci in /Users/Saeed/anaconda/lib/python2.7/site-packages (from dragnet)\n",
      "Requirement already satisfied: coverage in /Users/Saeed/anaconda/lib/python2.7/site-packages (from mozsci->dragnet)\n",
      "Requirement already satisfied: six in /Users/Saeed/anaconda/lib/python2.7/site-packages (from mozsci->dragnet)\n",
      "Requirement already satisfied: simplejson in /Users/Saeed/anaconda/lib/python2.7/site-packages (from mozsci->dragnet)\n",
      "Requirement already satisfied: nose in /Users/Saeed/anaconda/lib/python2.7/site-packages (from mozsci->dragnet)\n",
      "Building wheels for collected packages: dragnet\n",
      "  Running setup.py bdist_wheel for dragnet ... \u001b[?25lerror\n",
      "  Complete output from command /Users/Saeed/anaconda/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/16/wh2c3p4d6ks6h6ptsmlmw0ym0000gn/T/pip-build-MEZrvp/dragnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /var/folders/16/wh2c3p4d6ks6h6ptsmlmw0ym0000gn/T/tmpsifb5Vpip-wheel- --python-tag cp27:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.7-x86_64-2.7\n",
      "  creating build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/__init__.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/compat.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/content_extraction_model.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/data_processing.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/features.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/kmeans.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/kohlschuetter.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/model_training.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/models.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/util.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  copying dragnet/weninger.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "  creating build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models\n",
      "  creating build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.15.2_0.17.1\n",
      "  copying dragnet/pickled_models/sklearn_0.15.2_0.17.1/kohlschuetter_weninger_readability_content_comments_model.pickle.gz -> build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.15.2_0.17.1\n",
      "  copying dragnet/pickled_models/sklearn_0.15.2_0.17.1/kohlschuetter_weninger_readability_content_model.pickle.gz -> build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.15.2_0.17.1\n",
      "  creating build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.18.0\n",
      "  copying dragnet/pickled_models/sklearn_0.18.0/kohlschuetter_weninger_readability_content_comments_model.pickle.gz -> build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.18.0\n",
      "  copying dragnet/pickled_models/sklearn_0.18.0/kohlschuetter_weninger_readability_content_model.pickle.gz -> build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.18.0\n",
      "  running build_ext\n",
      "  skipping 'dragnet/_weninger.cpp' Cython extension (up-to-date)\n",
      "  building 'dragnet._weninger' extension\n",
      "  creating build/temp.macosx-10.7-x86_64-2.7\n",
      "  creating build/temp.macosx-10.7-x86_64-2.7/dragnet\n",
      "  gcc -fno-strict-aliasing -I/Users/Saeed/anaconda/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include -I/Users/Saeed/anaconda/include/python2.7 -c dragnet/_weninger.cpp -o build/temp.macosx-10.7-x86_64-2.7/dragnet/_weninger.o\n",
      "  In file included from dragnet/_weninger.cpp:251:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:18:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1809:\n",
      "  /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: \"Using deprecated NumPy API, disable it by \"          \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n",
      "  #warning \"Using deprecated NumPy API, disable it by \" \\\n",
      "   ^\n",
      "  dragnet/_weninger.cpp:6228:28: warning: unused function '__Pyx_PyObject_AsString' [-Wunused-function]\n",
      "  static CYTHON_INLINE char* __Pyx_PyObject_AsString(PyObject* o) {\n",
      "                             ^\n",
      "  dragnet/_weninger.cpp:6225:32: warning: unused function '__Pyx_PyUnicode_FromString' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject* __Pyx_PyUnicode_FromString(const char* c_str) {\n",
      "                                 ^\n",
      "  dragnet/_weninger.cpp:339:29: warning: unused function '__Pyx_Py_UNICODE_strlen' [-Wunused-function]\n",
      "  static CYTHON_INLINE size_t __Pyx_Py_UNICODE_strlen(const Py_UNICODE *u)\n",
      "                              ^\n",
      "  dragnet/_weninger.cpp:6340:33: warning: unused function '__Pyx_PyIndex_AsSsize_t' [-Wunused-function]\n",
      "  static CYTHON_INLINE Py_ssize_t __Pyx_PyIndex_AsSsize_t(PyObject* b) {\n",
      "                                  ^\n",
      "  dragnet/_weninger.cpp:6402:33: warning: unused function '__Pyx_PyInt_FromSize_t' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject * __Pyx_PyInt_FromSize_t(size_t ival) {\n",
      "                                  ^\n",
      "  dragnet/_weninger.cpp:5414:32: warning: unused function '__Pyx_PyInt_From_long' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject* __Pyx_PyInt_From_long(long value) {\n",
      "                                 ^\n",
      "  dragnet/_weninger.cpp:5651:48: warning: unused function '__pyx_t_float_complex_from_parts' [-Wunused-function]\n",
      "      static CYTHON_INLINE __pyx_t_float_complex __pyx_t_float_complex_from_parts(float x, float y) {\n",
      "                                                 ^\n",
      "  dragnet/_weninger.cpp:5771:49: warning: unused function '__pyx_t_double_complex_from_parts' [-Wunused-function]\n",
      "      static CYTHON_INLINE __pyx_t_double_complex __pyx_t_double_complex_from_parts(double x, double y) {\n",
      "                                                  ^\n",
      "  dragnet/_weninger.cpp:5915:27: warning: function '__Pyx_PyInt_As_long' is not needed and will not be emitted [-Wunneeded-internal-declaration]\n",
      "  static CYTHON_INLINE long __Pyx_PyInt_As_long(PyObject *x) {\n",
      "                            ^\n",
      "  dragnet/_weninger.cpp:2646:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew1' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew1(PyObject *__pyx_v_a) {\n",
      "                                 ^\n",
      "  dragnet/_weninger.cpp:2696:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew2' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew2(PyObject *__pyx_v_a, PyObject *__pyx_v_b) {\n",
      "                                 ^\n",
      "  dragnet/_weninger.cpp:2746:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew3' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew3(PyObject *__pyx_v_a, PyObject *__pyx_v_b, PyObject *__pyx_v_c) {\n",
      "                                 ^\n",
      "  dragnet/_weninger.cpp:2796:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew4' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew4(PyObject *__pyx_v_a, PyObject *__pyx_v_b, PyObject *__pyx_v_c, PyObject *__pyx_v_d) {\n",
      "                                 ^\n",
      "  dragnet/_weninger.cpp:2846:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew5' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew5(PyObject *__pyx_v_a, PyObject *__pyx_v_b, PyObject *__pyx_v_c, PyObject *__pyx_v_d, PyObject *__pyx_v_e) {\n",
      "                                 ^\n",
      "  dragnet/_weninger.cpp:3654:27: warning: unused function '__pyx_f_5numpy_set_array_base' [-Wunused-function]\n",
      "  static CYTHON_INLINE void __pyx_f_5numpy_set_array_base(PyArrayObject *__pyx_v_arr, PyObject *__pyx_v_base) {\n",
      "                            ^\n",
      "  dragnet/_weninger.cpp:3750:32: warning: unused function '__pyx_f_5numpy_get_array_base' [-Wunused-function]\n",
      "  static CYTHON_INLINE PyObject *__pyx_f_5numpy_get_array_base(PyArrayObject *__pyx_v_arr) {\n",
      "                                 ^\n",
      "  17 warnings generated.\n",
      "  g++ -bundle -undefined dynamic_lookup -L/Users/Saeed/anaconda/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-2.7/dragnet/_weninger.o -L/Users/Saeed/anaconda/lib -o build/lib.macosx-10.7-x86_64-2.7/dragnet/_weninger.so\n",
      "  clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\n",
      "  cythoning dragnet/lcs.pyx to dragnet/lcs.cpp\n",
      "  building 'dragnet.lcs' extension\n",
      "  gcc -fno-strict-aliasing -I/Users/Saeed/anaconda/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include -I/Users/Saeed/anaconda/include/python2.7 -c dragnet/lcs.cpp -o build/temp.macosx-10.7-x86_64-2.7/dragnet/lcs.o\n",
      "  In file included from dragnet/lcs.cpp:449:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:18:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1809:\n",
      "  /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: \"Using deprecated NumPy API, disable it by \"          \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n",
      "  #warning \"Using deprecated NumPy API, disable it by \" \\\n",
      "   ^\n",
      "  1 warning generated.\n",
      "  g++ -bundle -undefined dynamic_lookup -L/Users/Saeed/anaconda/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-2.7/dragnet/lcs.o -L/Users/Saeed/anaconda/lib -o build/lib.macosx-10.7-x86_64-2.7/dragnet/lcs.so\n",
      "  clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\n",
      "  cythoning dragnet/blocks.pyx to dragnet/blocks.cpp\n",
      "  warning: dragnet/blocks.pyx:172:29: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\n",
      "  warning: dragnet/blocks.pyx:172:36: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\n",
      "  warning: dragnet/blocks.pyx:586:33: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\n",
      "  warning: dragnet/blocks.pyx:586:40: Non-trivial type declarators in shared declaration (e.g. mix of pointers and values). Each pointer declaration should be on its own line.\n",
      "  building 'dragnet.blocks' extension\n",
      "  gcc -fno-strict-aliasing -I/Users/Saeed/anaconda/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/lxml/includes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/lxml -I/usr/include/libxml2 -I/Users/Saeed/anaconda/include/python2.7 -c dragnet/blocks.cpp -o build/temp.macosx-10.7-x86_64-2.7/dragnet/blocks.o\n",
      "  dragnet/blocks.cpp:2063:13: warning: unused function '__pyx_f_7dragnet_6blocks_empty_callback' [-Wunused-function]\n",
      "  static void __pyx_f_7dragnet_6blocks_empty_callback(CYTHON_UNUSED struct __pyx_obj_7dragnet_6blocks_PartialBlock *__pyx_v_pb, CYTHON_UNUSED std::string __pyx_v_x) {\n",
      "              ^\n",
      "  1 warning generated.\n",
      "  g++ -bundle -undefined dynamic_lookup -L/Users/Saeed/anaconda/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-2.7/dragnet/blocks.o -L/Users/Saeed/anaconda/lib -lxml2 -o build/lib.macosx-10.7-x86_64-2.7/dragnet/blocks.so\n",
      "  clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\n",
      "  skipping 'dragnet/readability.cpp' Cython extension (up-to-date)\n",
      "  building 'dragnet.readability' extension\n",
      "  gcc -fno-strict-aliasing -I/Users/Saeed/anaconda/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include -I/Users/Saeed/anaconda/include/python2.7 -c dragnet/readability.cpp -o build/temp.macosx-10.7-x86_64-2.7/dragnet/readability.o -std=c++0x\n",
      "  In file included from dragnet/readability.cpp:449:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:18:\n",
      "  In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1809:\n",
      "  /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: \"Using deprecated NumPy API, disable it by \"          \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n",
      "  #warning \"Using deprecated NumPy API, disable it by \" \\\n",
      "   ^\n",
      "  In file included from dragnet/readability.cpp:459:\n",
      "  dragnet/_readability.cc:4:10: fatal error: 'unordered_map' file not found\n",
      "  #include <unordered_map>\n",
      "           ^\n",
      "  1 warning and 1 error generated.\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for dragnet\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for dragnet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to build dragnet\n",
      "Installing collected packages: dragnet\n",
      "  Running setup.py install for dragnet ... \u001b[?25lerror\n",
      "    Complete output from command /Users/Saeed/anaconda/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/16/wh2c3p4d6ks6h6ptsmlmw0ym0000gn/T/pip-build-MEZrvp/dragnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /var/folders/16/wh2c3p4d6ks6h6ptsmlmw0ym0000gn/T/pip-J6kuhO-record/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-10.7-x86_64-2.7\n",
      "    creating build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/__init__.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/compat.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/content_extraction_model.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/data_processing.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/features.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/kmeans.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/kohlschuetter.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/model_training.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/models.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/util.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    copying dragnet/weninger.py -> build/lib.macosx-10.7-x86_64-2.7/dragnet\n",
      "    creating build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models\n",
      "    creating build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.15.2_0.17.1\n",
      "    copying dragnet/pickled_models/sklearn_0.15.2_0.17.1/kohlschuetter_weninger_readability_content_comments_model.pickle.gz -> build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.15.2_0.17.1\n",
      "    copying dragnet/pickled_models/sklearn_0.15.2_0.17.1/kohlschuetter_weninger_readability_content_model.pickle.gz -> build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.15.2_0.17.1\n",
      "    creating build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.18.0\n",
      "    copying dragnet/pickled_models/sklearn_0.18.0/kohlschuetter_weninger_readability_content_comments_model.pickle.gz -> build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.18.0\n",
      "    copying dragnet/pickled_models/sklearn_0.18.0/kohlschuetter_weninger_readability_content_model.pickle.gz -> build/lib.macosx-10.7-x86_64-2.7/dragnet/pickled_models/sklearn_0.18.0\n",
      "    running build_ext\n",
      "    skipping 'dragnet/_weninger.cpp' Cython extension (up-to-date)\n",
      "    building 'dragnet._weninger' extension\n",
      "    creating build/temp.macosx-10.7-x86_64-2.7\n",
      "    creating build/temp.macosx-10.7-x86_64-2.7/dragnet\n",
      "    gcc -fno-strict-aliasing -I/Users/Saeed/anaconda/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include -I/Users/Saeed/anaconda/include/python2.7 -c dragnet/_weninger.cpp -o build/temp.macosx-10.7-x86_64-2.7/dragnet/_weninger.o\n",
      "    In file included from dragnet/_weninger.cpp:251:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:18:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1809:\n",
      "    /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: \"Using deprecated NumPy API, disable it by \"          \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n",
      "    #warning \"Using deprecated NumPy API, disable it by \" \\\n",
      "     ^\n",
      "    dragnet/_weninger.cpp:6228:28: warning: unused function '__Pyx_PyObject_AsString' [-Wunused-function]\n",
      "    static CYTHON_INLINE char* __Pyx_PyObject_AsString(PyObject* o) {\n",
      "                               ^\n",
      "    dragnet/_weninger.cpp:6225:32: warning: unused function '__Pyx_PyUnicode_FromString' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject* __Pyx_PyUnicode_FromString(const char* c_str) {\n",
      "                                   ^\n",
      "    dragnet/_weninger.cpp:339:29: warning: unused function '__Pyx_Py_UNICODE_strlen' [-Wunused-function]\n",
      "    static CYTHON_INLINE size_t __Pyx_Py_UNICODE_strlen(const Py_UNICODE *u)\n",
      "                                ^\n",
      "    dragnet/_weninger.cpp:6340:33: warning: unused function '__Pyx_PyIndex_AsSsize_t' [-Wunused-function]\n",
      "    static CYTHON_INLINE Py_ssize_t __Pyx_PyIndex_AsSsize_t(PyObject* b) {\n",
      "                                    ^\n",
      "    dragnet/_weninger.cpp:6402:33: warning: unused function '__Pyx_PyInt_FromSize_t' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject * __Pyx_PyInt_FromSize_t(size_t ival) {\n",
      "                                    ^\n",
      "    dragnet/_weninger.cpp:5414:32: warning: unused function '__Pyx_PyInt_From_long' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject* __Pyx_PyInt_From_long(long value) {\n",
      "                                   ^\n",
      "    dragnet/_weninger.cpp:5651:48: warning: unused function '__pyx_t_float_complex_from_parts' [-Wunused-function]\n",
      "        static CYTHON_INLINE __pyx_t_float_complex __pyx_t_float_complex_from_parts(float x, float y) {\n",
      "                                                   ^\n",
      "    dragnet/_weninger.cpp:5771:49: warning: unused function '__pyx_t_double_complex_from_parts' [-Wunused-function]\n",
      "        static CYTHON_INLINE __pyx_t_double_complex __pyx_t_double_complex_from_parts(double x, double y) {\n",
      "                                                    ^\n",
      "    dragnet/_weninger.cpp:5915:27: warning: function '__Pyx_PyInt_As_long' is not needed and will not be emitted [-Wunneeded-internal-declaration]\n",
      "    static CYTHON_INLINE long __Pyx_PyInt_As_long(PyObject *x) {\n",
      "                              ^\n",
      "    dragnet/_weninger.cpp:2646:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew1' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew1(PyObject *__pyx_v_a) {\n",
      "                                   ^\n",
      "    dragnet/_weninger.cpp:2696:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew2' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew2(PyObject *__pyx_v_a, PyObject *__pyx_v_b) {\n",
      "                                   ^\n",
      "    dragnet/_weninger.cpp:2746:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew3' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew3(PyObject *__pyx_v_a, PyObject *__pyx_v_b, PyObject *__pyx_v_c) {\n",
      "                                   ^\n",
      "    dragnet/_weninger.cpp:2796:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew4' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew4(PyObject *__pyx_v_a, PyObject *__pyx_v_b, PyObject *__pyx_v_c, PyObject *__pyx_v_d) {\n",
      "                                   ^\n",
      "    dragnet/_weninger.cpp:2846:32: warning: unused function '__pyx_f_5numpy_PyArray_MultiIterNew5' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject *__pyx_f_5numpy_PyArray_MultiIterNew5(PyObject *__pyx_v_a, PyObject *__pyx_v_b, PyObject *__pyx_v_c, PyObject *__pyx_v_d, PyObject *__pyx_v_e) {\n",
      "                                   ^\n",
      "    dragnet/_weninger.cpp:3654:27: warning: unused function '__pyx_f_5numpy_set_array_base' [-Wunused-function]\n",
      "    static CYTHON_INLINE void __pyx_f_5numpy_set_array_base(PyArrayObject *__pyx_v_arr, PyObject *__pyx_v_base) {\n",
      "                              ^\n",
      "    dragnet/_weninger.cpp:3750:32: warning: unused function '__pyx_f_5numpy_get_array_base' [-Wunused-function]\n",
      "    static CYTHON_INLINE PyObject *__pyx_f_5numpy_get_array_base(PyArrayObject *__pyx_v_arr) {\n",
      "                                   ^\n",
      "    17 warnings generated.\n",
      "    g++ -bundle -undefined dynamic_lookup -L/Users/Saeed/anaconda/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-2.7/dragnet/_weninger.o -L/Users/Saeed/anaconda/lib -o build/lib.macosx-10.7-x86_64-2.7/dragnet/_weninger.so\n",
      "    clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\n",
      "    skipping 'dragnet/lcs.cpp' Cython extension (up-to-date)\n",
      "    building 'dragnet.lcs' extension\n",
      "    gcc -fno-strict-aliasing -I/Users/Saeed/anaconda/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include -I/Users/Saeed/anaconda/include/python2.7 -c dragnet/lcs.cpp -o build/temp.macosx-10.7-x86_64-2.7/dragnet/lcs.o\n",
      "    In file included from dragnet/lcs.cpp:449:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:18:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1809:\n",
      "    /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: \"Using deprecated NumPy API, disable it by \"          \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n",
      "    #warning \"Using deprecated NumPy API, disable it by \" \\\n",
      "     ^\n",
      "    1 warning generated.\n",
      "    g++ -bundle -undefined dynamic_lookup -L/Users/Saeed/anaconda/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-2.7/dragnet/lcs.o -L/Users/Saeed/anaconda/lib -o build/lib.macosx-10.7-x86_64-2.7/dragnet/lcs.so\n",
      "    clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\n",
      "    skipping 'dragnet/blocks.cpp' Cython extension (up-to-date)\n",
      "    building 'dragnet.blocks' extension\n",
      "    gcc -fno-strict-aliasing -I/Users/Saeed/anaconda/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/lxml/includes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/lxml -I/usr/include/libxml2 -I/Users/Saeed/anaconda/include/python2.7 -c dragnet/blocks.cpp -o build/temp.macosx-10.7-x86_64-2.7/dragnet/blocks.o\n",
      "    dragnet/blocks.cpp:2063:13: warning: unused function '__pyx_f_7dragnet_6blocks_empty_callback' [-Wunused-function]\n",
      "    static void __pyx_f_7dragnet_6blocks_empty_callback(CYTHON_UNUSED struct __pyx_obj_7dragnet_6blocks_PartialBlock *__pyx_v_pb, CYTHON_UNUSED std::string __pyx_v_x) {\n",
      "                ^\n",
      "    1 warning generated.\n",
      "    g++ -bundle -undefined dynamic_lookup -L/Users/Saeed/anaconda/lib -arch x86_64 -arch x86_64 build/temp.macosx-10.7-x86_64-2.7/dragnet/blocks.o -L/Users/Saeed/anaconda/lib -lxml2 -o build/lib.macosx-10.7-x86_64-2.7/dragnet/blocks.so\n",
      "    clang: warning: libstdc++ is deprecated; move to libc++ with a minimum deployment target of OS X 10.9 [-Wdeprecated]\n",
      "    skipping 'dragnet/readability.cpp' Cython extension (up-to-date)\n",
      "    building 'dragnet.readability' extension\n",
      "    gcc -fno-strict-aliasing -I/Users/Saeed/anaconda/include -arch x86_64 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include -I/Users/Saeed/anaconda/include/python2.7 -c dragnet/readability.cpp -o build/temp.macosx-10.7-x86_64-2.7/dragnet/readability.o -std=c++0x\n",
      "    In file included from dragnet/readability.cpp:449:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/arrayobject.h:4:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:18:\n",
      "    In file included from /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1809:\n",
      "    /Users/Saeed/anaconda/lib/python2.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:15:2: warning: \"Using deprecated NumPy API, disable it by \"          \"#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-W#warnings]\n",
      "    #warning \"Using deprecated NumPy API, disable it by \" \\\n",
      "     ^\n",
      "    In file included from dragnet/readability.cpp:459:\n",
      "    dragnet/_readability.cc:4:10: fatal error: 'unordered_map' file not found\n",
      "    #include <unordered_map>\n",
      "             ^\n",
      "    1 warning and 1 error generated.\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/Users/Saeed/anaconda/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/16/wh2c3p4d6ks6h6ptsmlmw0ym0000gn/T/pip-build-MEZrvp/dragnet/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /var/folders/16/wh2c3p4d6ks6h6ptsmlmw0ym0000gn/T/pip-J6kuhO-record/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/16/wh2c3p4d6ks6h6ptsmlmw0ym0000gn/T/pip-build-MEZrvp/dragnet/\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Nedd to be done only once\n",
    "\n",
    "# !pip install jupyter\n",
    "#!pip install ijson\n",
    "# !pip install pandas\n",
    "\n",
    "#!pip install pandas_datareader\n",
    "#!pip install httplib2\n",
    "# !pip install watson_developer_cloud\n",
    "# !apt-get install build-essential libssl-dev libffi-dev python-dev\n",
    "#!pip install quandl\n",
    "# !apt-get install python-matplotlib\n",
    "#!pip install nytimesarticle\n",
    "# !apt-get install python-lxml\n",
    "# !pip install cython\n",
    "# !apt-get install python-scipy\n",
    "# !pip install scikit-learn\n",
    "#!apt-get install libxml2-dev libxmlsec1-dev\n",
    "!pip install dragnet\n",
    "#### !pip install tensorflow\n",
    "\n",
    "# Install CUDA and CuDNN\n",
    "#\n",
    "# wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2v2_8.0.61-1_ppc64el-deb\n",
    "# dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2v2_8.0.61-1_ppc64el-deb\n",
    "# apt-get update\n",
    "# apt-get install cuda\n",
    "#\n",
    "# Download cuDNN from https://developer.nvidia.com/rdp/cudnn-download  (libcudnn5_5.1.10-1+cuda8.0_ppc64el.deb)\n",
    "# Note: requires a login\n",
    "#\n",
    "# dpkg -i libcudnn5_5.1.10-1+cuda8.0_ppc64el.deb\n",
    "#\n",
    "# Install Tensorflow\n",
    "#\n",
    "# Download the latest mldl-repo-local .deb file from wget https://download.boulder.ibm.com/ibmdl/pub/software/server/mldl/mldl-repo-local_3.3.0_ppc64el.deb\n",
    "# dpkg -i mldl-repo-local*.deb\n",
    "# apt-get update\n",
    "# apt-get install tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, import necessary libraries. All these libraries are installed using 'pip install <package name>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named dragnet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0b55c1434180>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# api = articleAPI('e232f0c680594b489111a6e7c41cba2c')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdragnet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontent_extractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_comments_extractor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named dragnet"
     ]
    }
   ],
   "source": [
    "import StringIO\n",
    "import json\n",
    "import ijson\n",
    "import sys\n",
    "sys.path.insert(0, \"/opt/DL/tensorflow/lib/python2.7/site-packages/\")\n",
    "import os\n",
    "import requests\n",
    "import random\n",
    "import httplib2\n",
    "import urllib2 \n",
    "from base64 import b64encode\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import datetime\n",
    "import time\n",
    "import calendar\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from pandas.tseries.offsets import *\n",
    "\n",
    "import time\n",
    "\n",
    "from watson_developer_cloud import AlchemyDataNewsV1\n",
    "import quandl\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from nytimesarticle import articleAPI\n",
    "# api = articleAPI('e232f0c680594b489111a6e7c41cba2c')\n",
    "\n",
    "from dragnet import content_extractor, content_comments_extractor\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Get, merge and unify Stock Market data\n",
    "In this current experiment, the market data are coming from manually retrieved \".csv\" files as some historical data are not available on the public finance websites. If you have a Bloomberg account you can retrieve all the data using the Bloomberg Python API: https://www.bloomberglabs.com/api/libraries/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = datetime.date(2014, 1, 2)\n",
    "end = datetime.date(2017, 2, 13)\n",
    "# print '{:%Y-%m-%d}'.format(start)\n",
    "# print '{:%Y-%m-%d}'.format(end)\n",
    "\n",
    "# create the index\n",
    "all_days = pd.date_range(start, end, freq='D')\n",
    "\n",
    "# Australian Clean Tech Index (asx_cti)\n",
    "ctius = pd.read_csv('CTIUS.csv')\n",
    "ctius = ctius.set_index(pd.to_datetime(ctius['Date']))\n",
    "\n",
    "# Nasdaq Clean Edge Green Energy Index\n",
    "# cels = pd.read_csv('cels.csv', index_col='Date')\n",
    "cels = pd.read_csv('cels.csv')\n",
    "cels = cels.set_index(pd.to_datetime(cels['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(ctius, cels, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# FTSE, S&P, DAX Clean Energy Index\n",
    "# europe_index = pd.read_csv('DAX-SP-FTSE-index.csv', index_col='Date')\n",
    "europe_index = pd.read_csv('DAX-SP-FTSE-index.csv')\n",
    "europe_index = europe_index.set_index(pd.to_datetime(europe_index['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(data, europe_index, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# drop the extra 'Date' columns created during the merge\n",
    "data.drop('Date_x', axis=1, inplace=True)\n",
    "data.drop('Date_y', axis=1, inplace=True)\n",
    "\n",
    "# Credit Suisse\n",
    "n8wh = pd.read_csv('n8wh.csv')\n",
    "n8wh = n8wh.set_index(pd.to_datetime(n8wh['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(data, n8wh, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# Nasdaq Clean Edge\n",
    "qcln = pd.read_csv('qcln.csv')\n",
    "qcln = qcln.set_index(pd.to_datetime(qcln['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(data, qcln, left_index=True, right_index=True, how='outer')\n",
    "data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# iSahres S&P Clean Energy\n",
    "icln = pd.read_csv('icln.csv')\n",
    "icln = icln.set_index(pd.to_datetime(icln['Date']))\n",
    "\n",
    "# Merge the data \n",
    "data = pd.merge(data, icln, left_index=True, right_index=True, how='outer')\n",
    "data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Pandas includes very convenient functions for filling gaps in the data.\n",
    "# Stock values during the week end or holidays are not available\n",
    "# so we need to fill the gaps\n",
    "data = data.interpolate(method='linear', axis=0).ffill().bfill()\n",
    "\n",
    "# Equity Uncertainty INDEX (dei)\n",
    "eui = pd.read_csv('eui.csv')\n",
    "eui = eui.set_index(pd.to_datetime(eui['Date']))\n",
    "\n",
    "# Merge the data and do some cleanup\n",
    "data = pd.merge(data, eui, left_index=True, right_index=True, how='outer')\n",
    "data.drop('Date_x', axis=1, inplace=True)\n",
    "data.drop('Date_y', axis=1, inplace=True)\n",
    "data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# Rename column\n",
    "data=data.rename(columns = {'Daily Equity Index':'dei'})\n",
    "\n",
    "data.tail(5)\n",
    "# print data.index.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "At this point,we've sourced 3+ years of time series for financial indices, combined the pertinent data into a single data structure, and harmonized the data to have the same number of entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Get New York Times data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2,461 'Alternative and Renewable Energy' articles are going to be retrieved from the New York Times database by the cell below. As it takes time due to NYT API limitations, the process has been ran once and the data have been stored in JSON format in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.date(2014, 1, 2)\n",
    "end = datetime.date(2017, 2, 13)\n",
    "# print '{:%Y-%m-%d}'.format(start)\n",
    "# print '{:%Y-%m-%d}'.format(end)\n",
    "# print 'Start processing ....'\n",
    "\n",
    "if os.path.isfile('nyt_article_results.json'):\n",
    "    print 'Data gathered...'\n",
    "else:\n",
    "    all_articles = []\n",
    "    payload = {'fq': 'subject.contains:(\"Alternative and Renewable Energy\",\"Global Warming\",\"Solar Energy\",\"Wind Power\",\"Greenhouse Gas Emissions\")' ,'begin_date': '20140102' , 'end_date': '20170213'}\n",
    "    r = requests.get('https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=<your key>', params=payload)\n",
    "    data = json.loads(r.text)\n",
    "    # print data\n",
    "    total_hits=data['response']['meta']['hits']\n",
    "    print 'Number of articles to process: '+str(total_hits)\n",
    "    time.sleep(10)\n",
    "    counter = 0\n",
    "    while counter < (total_hits/10+1):\n",
    "      print 'Processing '+str(counter)\n",
    "      r = requests.get('https://api.nytimes.com/svc/search/v2/articlesearch.json?api-key=<your key>', params={'fq': 'subject.contains:(\"Alternative and Renewable Energy\",\"Global Warming\",\"Solar Energy\",\"Wind Power\",\"Greenhouse Gas Emissions\")' ,'begin_date': '20140102' , 'end_date': '20170213', 'fl': 'web_url,pub_date'  ,'sort': 'oldest', 'page': counter})\n",
    "      time.sleep(10)\n",
    "      counter = counter + 1\n",
    "      data = json.loads(r.text)\n",
    "      with open('nyt_article_temp.json', 'a') as f:\n",
    "          f.write(json.dumps(data,indent=2))\n",
    "          f.close()  \n",
    "      for a_doc in data['response']['docs']:\n",
    "        url = a_doc['web_url']\n",
    "        publication_date = a_doc['pub_date']\n",
    "        a_articles_dict = {\"URL\": url, \"Publication Date\" : publication_date}\n",
    "        all_articles.append(a_articles_dict)\n",
    "    with open('nyt_article_results.json', 'w') as f:\n",
    "      f.write(json.dumps(all_articles,indent=2))\n",
    "      f.close()\n",
    "\n",
    "nyt = pd.read_json('nyt_article_results.json')       \n",
    "nyt.tail(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Get Sentiment Analysis from Watson "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next step is to get the result of the Watson sentiment analysis for each of the arcticles. The   IBM Watson Natural Language Understanding service takes as input text or url. However the Watson service complains when the article has too many links so we add another step, using the Python Dragnet package to purge the HTML formating and send the content of the link as text to Watson. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "}\n",
    "\n",
    "def watson_request(content):\n",
    "    data = '{\"text\": \"'+content+'\",\"features\": { \"entities\": { \"sentiment\": true, \"limit\": 1 }}}'\n",
    "    response = requests.post('https://gateway.watsonplatform.net/natural-language-understanding/api/v1/analyze?version=2017-02-27', headers=headers, data=data, auth=('<password>', '<user id>'))\n",
    "    # return json.dumps(response.json(), indent=4)\n",
    "    return response.text\n",
    "\n",
    "if os.path.isfile('watson-results.csv'):\n",
    "    print 'Data processed...'\n",
    "else:\n",
    "    res = json.loads(open('nyt_article_results.json').read())\n",
    "    counter = 0\n",
    "\n",
    "    articles = []\n",
    "    for result in res:\n",
    "         # print 'Processing '+ str(counter)\n",
    "         dic = {}\n",
    "         dic['url'] = result['web_url']\n",
    "         dic['date'] = result['pub_date']\n",
    "         # fetch HTML\n",
    "         url = result['web_url']\n",
    "         # print url\n",
    "    \n",
    "         r = requests.get(url)\n",
    "         # get main article without comments\n",
    "         content = content_extractor.analyze(r.content) \n",
    "         response = watson_request(content)\n",
    "         # print response\n",
    "     \n",
    "         # check if a 'score' field is returned\n",
    "         if \"score\" not in response:    \n",
    "            score = 0\n",
    "         else:   \n",
    "             resp_dict = json.loads(response)['entities']\n",
    "             # print resp_dict\n",
    "             n = json.dumps(resp_dict)  \n",
    "             val = json.loads(n) \n",
    "             # print val\n",
    "             score = val[0]['sentiment']['score']\n",
    "     \n",
    "         if score < 0:   \n",
    "             dic['negative'] = abs(score)\n",
    "             dic['positive'] = 0\n",
    "         else:\n",
    "             dic['positive'] = score\n",
    "             dic['negative'] = 0\n",
    "         articles.append(dic)\n",
    "    \n",
    "         counter = counter + 1\n",
    "         time.sleep(1)\n",
    "    \n",
    "    nyt_df = pd.DataFrame(articles) \n",
    "    nyt_df['index'] = pd.to_datetime(nyt_df['date'])\n",
    "\n",
    "    # set index\n",
    "    nyt_df = nyt_df.set_index(['index'])\n",
    "\n",
    "    nyt_df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    # Save dataframe to file\n",
    "    nyt_df.to_csv('watson-results.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "# Load the file previously processed by the Watson Service.\n",
    "watson_data = pd.read_csv('watson-results.csv', sep='\\t')\n",
    "\n",
    "temp_data = watson_data\n",
    "\n",
    "# format the dataframe to get the date as the index\n",
    "watson_data['index'] = pd.to_datetime(watson_data['index'])\n",
    "# set index\n",
    "watson_data = watson_data.set_index(['index'])\n",
    "\n",
    "# resample based on the date\n",
    "# watson_data = watson_data.resample('D').mean()\n",
    "watson_data = watson_data.resample('D').mean()\n",
    "# change NaN by zeros\n",
    "watson_data = watson_data.fillna(0)\n",
    "# Replace 0 by .0000001 to avoid divide-by-zero errors\n",
    "watson_data = watson_data.replace(0,.000001)\n",
    "\n",
    "watson_data.to_csv('watson.csv', sep=' ')\n",
    "\n",
    "# merge \n",
    "data = pd.merge(data, watson_data, left_index=True, right_index=True, how='outer')\n",
    "\n",
    "# watson_data.head(10) \n",
    "temp_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Exploratory Data Analysis (EDA) is foundational to working with machine learning, and any other sort of analysis. EDA is about understanding the assumptions and why we are making those assumptions. \n",
    "First, take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can see that the various indices operate on scales differing by orders of magnitude. It's best to scale the data so that, for example, operations involving multiple indices aren't unduly influenced by a single, massive index.\n",
    "\n",
    "Plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# N.B. A super-useful trick-ette is to assign the return value of plot to _ \n",
    "# so that you don't get text printed before the plot itself.\n",
    "\n",
    "_ = pd.concat([data['asx_cti'],\n",
    "  data['sp_gtced'],\n",
    "  data['dax_eusdn'],\n",
    "  data['ftse_eo100'],\n",
    "  data['cels'],\n",
    "  data['n8wh'],\n",
    "  data['qcln'],\n",
    "  data['icln'],\n",
    "  data['dei'],               \n",
    "  data['negative'],\n",
    "  data['positive']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As expected, the structure isn't uniformly visible for the indices. Divide each value in an individual index by the maximum value for that index., and then replot. The maximum value of all indices will be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "data['asx_cti_scaled'] = data['asx_cti'] / max(data['asx_cti'])\n",
    "data['sp_gtced_scaled'] = data['sp_gtced'] / max(data['sp_gtced'])\n",
    "data['dax_eusdn_scaled'] = data['dax_eusdn'] / max(data['dax_eusdn'])\n",
    "data['ftse_eo100_scaled'] = data['ftse_eo100'] / max(data['ftse_eo100'])\n",
    "data['cels_scaled'] = data['cels'] / max(data['cels'])\n",
    "data['n8wh_scaled'] = data['n8wh'] / max(data['n8wh'])\n",
    "data['qcln_scaled'] = data['qcln'] / max(data['qcln'])\n",
    "data['icln_scaled'] = data['icln'] / max(data['icln'])\n",
    "data['dei_scaled'] = data['dei'] / max(data['dei'])\n",
    "data['positive_scaled'] = data['positive']\n",
    "data['negative_scaled'] = data['negative']\n",
    "\n",
    "\n",
    "# data['positive_scaled'] = data['positive'] / max(data['positive'])\n",
    "# data['negative_scaled'] = data['negative'] / max(data['negative'])\n",
    "\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([data['asx_cti_scaled'],\n",
    "  data['sp_gtced_scaled'],\n",
    "  data['dax_eusdn_scaled'],\n",
    "  data['ftse_eo100_scaled'],\n",
    "  data['cels_scaled'],\n",
    "  data['n8wh_scaled'],\n",
    "  data['qcln_scaled'],\n",
    "  data['icln_scaled'],\n",
    "  data['dei_scaled'],\n",
    "  data['positive_scaled'],\n",
    "  data['negative_scaled']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can see that, over the three-year period, these indices are correlated. Notice that sudden drops from economic events happened globally to all indices, and they otherwise exhibited general rises. This is an good start, though not the complete story. Next, plot autocorrelations for each of the indices. The autocorrelations determine correlations between current values of the index and lagged values of the same index. The goal is to determine whether the lagged values are reliable indicators of the current values. If they are, then we've identified a correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_figwidth(20)\n",
    "fig.set_figheight(15)\n",
    "\n",
    "_ = autocorrelation_plot(data['asx_cti'], label='asx_cti')\n",
    "_ = autocorrelation_plot(data['sp_gtced'], label='sp_gtced')\n",
    "_ = autocorrelation_plot(data['dax_eusdn'], label='dax_eusdn')\n",
    "_ = autocorrelation_plot(data['ftse_eo100'], label='ftse_eo100')\n",
    "_ = autocorrelation_plot(data['cels'], label='cels')\n",
    "_ = autocorrelation_plot(data['n8wh'], label='n8wh')\n",
    "_ = autocorrelation_plot(data['qcln'], label='qcln')\n",
    "_ = autocorrelation_plot(data['icln'], label='icln')\n",
    "_ = autocorrelation_plot(data['dei'], label='dei')\n",
    "_ = autocorrelation_plot(data['positive'], label='positive')\n",
    "_ = autocorrelation_plot(data['negative'], label='negative')\n",
    "\n",
    "_ = plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We see strong autocorrelations, positive for around 200 and 400 lagged days, depending of the indices, then going negative. This tells us something we should intuitively know: if an index is rising it tends to carry on rising, and vice-versa. It should be encouraging that what we see here conforms to what we know about financial markets.\n",
    "\n",
    "Next, look at a scatter matrix, showing everything plotted against everything, to see how indices are correlated with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = scatter_matrix(pd.concat([data['asx_cti_scaled'],\n",
    "  data['sp_gtced_scaled'],\n",
    "  data['dax_eusdn_scaled'],\n",
    "  data['ftse_eo100_scaled'],\n",
    "  data['cels_scaled'],\n",
    "  data['n8wh_scaled'],\n",
    "  data['qcln_scaled'],\n",
    "  data['icln_scaled'],\n",
    "  data['dei_scaled'],\n",
    "  data['positive'],                            \n",
    "  data['negative']], axis=1), figsize=(20, 20), diagonal='kde')  # kernel density estimate (KDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "You can see significant correlations, further evidence that the premise is workable and one market can be influenced by another. \n",
    "\n",
    "The actual value of an index is not that useful for modeling. It can be a useful indicator, but to get to the heart of the matter, we need a time series that is stationary in the mean, thus having no trend in the data. There are various ways of doing that, but they all essentially look at the difference between values, rather than the absolute value. In the case of market data, the usual practice is to work with logged returns, calculated as the natural logarithm of the index today divided by the index yesterday:\n",
    "\n",
    "`ln(Vt/Vt-1)`\n",
    "\n",
    "There are more reasons why the log return is preferable to the percent return (for example the log is normally distributed and additive), but they don't matter much for this work. What matters is to get to a stationary time series.\n",
    "\n",
    "Calculate and plot the log returns in a new DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "log_return_data = pd.DataFrame()\n",
    "\n",
    "log_return_data['asx_cti_log_return'] = np.log(data['asx_cti']/data['asx_cti'].shift())\n",
    "log_return_data['sp_gtced_log_return'] = np.log(data['sp_gtced']/data['sp_gtced'].shift())\n",
    "log_return_data['dax_eusdn_log_return'] = np.log(data['dax_eusdn']/data['dax_eusdn'].shift())\n",
    "log_return_data['ftse_eo100_log_return'] = np.log(data['ftse_eo100']/data['ftse_eo100'].shift())\n",
    "log_return_data['cels_log_return'] = np.log(data['cels']/data['cels'].shift())\n",
    "log_return_data['n8wh_log_return'] = np.log(data['n8wh']/data['n8wh'].shift())\n",
    "log_return_data['qcln_log_return'] = np.log(data['qcln']/data['qcln'].shift())\n",
    "log_return_data['icln_log_return'] = np.log(data['icln']/data['icln'].shift())\n",
    "log_return_data['dei_log_return'] = np.log(data['dei']/data['dei'].shift())\n",
    "log_return_data['positive_log_return'] = np.log(data['positive']/data['positive'].shift())\n",
    "log_return_data['negative_log_return'] = np.log(data['negative']/data['negative'].shift())\n",
    "\n",
    "# log_return_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Looking at the log returns, we see that the mean, min, max are all similar. We could go further and center the series on zero, scale them, and normalize the standard deviation, but there's no need to do that at this point. Let's move forward with plotting the data, and iterate if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([log_return_data['asx_cti_log_return'],\n",
    "  log_return_data['sp_gtced_log_return'],\n",
    "  log_return_data['dax_eusdn_log_return'],\n",
    "  log_return_data['ftse_eo100_log_return'],\n",
    "  log_return_data['cels_log_return']], axis=1).plot(figsize=(20, 15))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = pd.concat([log_return_data['n8wh_log_return'],\n",
    "  log_return_data['qcln_log_return'],\n",
    "  log_return_data['icln_log_return']], axis=1).plot(figsize=(20, 15))\n",
    "#  log_return_data['dei_log_return'],\n",
    "#  log_return_data['positive_log_return'],\n",
    "#  log_return_data['negative_log_return']], axis=1).plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can see from the plot that the log returns of our indices are similarly scaled and centered, with no visible trend in the data. It's looking good, so now look at autocorrelations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Look at a scatterplot to see how the log return indices correlate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "_ = scatter_matrix(log_return_data, figsize=(20, 20), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The story with the previous scatter plot for log returns is more subtle and more interesting. The US indices are strongly correlated, as expected. The other indices, less so, which is also expected. But there is structure and signal there. Now let's move forward and start to quantify it so we can start to choose features for our model.\n",
    "\n",
    "First look at how the log returns for the closing value of the Trust NASDAQ Cln Edge GrnEngyETF (QCLN) and/or iShares Global Clean Energy ETF (ICLN) correlate with the closing values of other indices available on the same day. This essentially means to assume the indices that close before the QCLN and ICLN (non-US indices) are available and the others (US indices) are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['qcln_0'] = log_return_data['qcln_log_return']     # same day\n",
    "tmp['asx_0'] = log_return_data['asx_cti_log_return']\n",
    "tmp['ftse_0'] = log_return_data['ftse_eo100_log_return']\n",
    "tmp['dax_0'] = log_return_data['dax_eusdn_log_return']\n",
    "tmp['n8wh_0'] = log_return_data['n8wh_log_return']\n",
    "tmp['icln_1'] = log_return_data['icln_log_return'].shift()  # can't get same day value so use previous day\n",
    "tmp['gtced_1'] = log_return_data['sp_gtced_log_return'].shift()\n",
    "tmp['cels_1'] = log_return_data['cels_log_return'].shift()\n",
    "tmp['dei_1'] = log_return_data['dei_log_return'].shift()\n",
    "tmp['positive_1'] = log_return_data['positive_log_return']\n",
    "tmp['negative_1'] = log_return_data['negative_log_return'].shift(2)\n",
    "\n",
    "tmp.corr().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, we are directly working with the premise. We're correlating the close of the Trust NASDAQ Cln Edge GrnEngyETF (QCLN) with signals available before the close of the US markets.  And we can see that the Trust NASDAQ Cln Edge GrnEngyETF (QCLN) close is correlated with the Austalian index at .72 which is a very strong correlation and the European indices at around 0.60 for the FTSE and DAX, which is also a strong correlation. \n",
    "\n",
    "Now look at how the log returns for the NASDAQ Cln Edge GrnEngyETF (QCLN) closing values correlate with index values from the previous day to see if they previous closing is predictive. Following from the premise that financial markets are Markov processes, there should be little or no value in historical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['qcln_0'] = log_return_data['qcln_log_return']   \n",
    "tmp['asx_0'] = log_return_data['asx_cti_log_return'].shift()\n",
    "tmp['ftse_0'] = log_return_data['ftse_eo100_log_return'].shift()\n",
    "tmp['dax_0'] = log_return_data['dax_eusdn_log_return'].shift()\n",
    "tmp['n8wh_0'] = log_return_data['n8wh_log_return'].shift()\n",
    "tmp['icln_1'] = log_return_data['icln_log_return'].shift(2)  \n",
    "tmp['gtced_1'] = log_return_data['sp_gtced_log_return'].shift(2)\n",
    "tmp['cels_1'] = log_return_data['cels_log_return'].shift(2)\n",
    "tmp['dei_1'] = log_return_data['dei_log_return'].shift(2)\n",
    "tmp['positive_1'] = log_return_data['positive_log_return'].shift(1)\n",
    "tmp['negative_1'] = log_return_data['negative_log_return'].shift(1)\n",
    "tmp.corr().iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We see little to no correlation in this data, meaning that yesterday's values are no practical help in predicting today's close. Let's go one step further and look at correlations between today and the the day before yesterday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Summing up the EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Summing up:\n",
    "\n",
    "* Australia index from the same day is a strong predictor for the Nasdaq Energy Index close.\n",
    "* European indices from the same day were a significant predictor for the Nasdaq Energy Index close.\n",
    "* Indices from previous days were not good predictors for the Nasdaq Energy Index close."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## PowerAI / TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "PowerAI is IBMs machine learning framework for companies that use servers based on its Power processors and NVIDIAs NVLink high-speed interconnects that allow for data to pass extremely quickly between the processor and the GPU that does most of the deep learning calculations. PowerAI now supports Googles popular Tensorflow machine learning library.\n",
    "[TensorFlow](https://tensorflow.org) is an open source software library, initiated by Google, for numerical computation using data flow graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Training and Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll use 80% of our data for training and 20% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "log_return_data['qcln_log_return_positive'] = 0\n",
    "log_return_data.ix[log_return_data['qcln_log_return'] >= 0, 'qcln_log_return_positive'] = 1\n",
    "log_return_data['qcln_log_return_negative'] = 0\n",
    "log_return_data.ix[log_return_data['qcln_log_return'] < 0, 'qcln_log_return_negative'] = 1\n",
    "\n",
    "training_test_data = pd.DataFrame(\n",
    "  columns=[\n",
    "    'qcln_log_return_positive', 'qcln_log_return_negative',\n",
    "    'qcln_log_return_1', 'qcln_log_return_2', 'qcln_log_return_3',\n",
    "    'icln_log_return_1', 'icln_log_return_2', 'icln_log_return_3',\n",
    "    'sp_gtced_log_return_1', 'sp_gtced_log_return_2', 'sp_gtced_log_return_3',\n",
    "    'cels_log_return_1', 'cels_log_return_2', 'cels_log_return_3',  \n",
    "    'asx_cti_log_return_0', 'asx_cti_log_return_1', 'asx_cti_log_return_2',\n",
    "    'ftse_eo100_log_return_0', 'ftse_eo100_log_return_1', 'ftse_eo100_log_return_2',  \n",
    "    'dax_eusdn_log_return_0', 'dax_eusdn_log_return_1', 'dax_eusdn_log_return_2', \n",
    "    'n8wh_log_return_0', 'n8wh_log_return_1', 'n8wh_log_return_2',\n",
    "    'dei_log_return_1', 'dei_log_return_2', 'dei_log_return_3',\n",
    "    'positive_log_return_0', 'positive_log_return_1', 'positive_log_return_2',\n",
    "    'negative_log_return_0', 'negative_log_return_1', 'negative_log_return_2'])\n",
    "\n",
    "for i in range(4, len(log_return_data)):\n",
    "  qcln_log_return_positive = log_return_data['qcln_log_return_positive'].ix[i]\n",
    "  qcln_log_return_negative = log_return_data['qcln_log_return_negative'].ix[i]\n",
    "  qcln_log_return_1 = log_return_data['qcln_log_return'].ix[i-1]\n",
    "  qcln_log_return_2 = log_return_data['qcln_log_return'].ix[i-2]\n",
    "  qcln_log_return_3 = log_return_data['qcln_log_return'].ix[i-3]\n",
    "  icln_log_return_1 = log_return_data['icln_log_return'].ix[i-1]\n",
    "  icln_log_return_2 = log_return_data['icln_log_return'].ix[i-2]\n",
    "  icln_log_return_3 = log_return_data['icln_log_return'].ix[i-3]\n",
    "  sp_gtced_log_return_1 = log_return_data['sp_gtced_log_return'].ix[i-1]\n",
    "  sp_gtced_log_return_2 = log_return_data['sp_gtced_log_return'].ix[i-2]\n",
    "  sp_gtced_log_return_3 = log_return_data['sp_gtced_log_return'].ix[i-3]\n",
    "  cels_log_return_1 = log_return_data['cels_log_return'].ix[i-1]\n",
    "  cels_log_return_2 = log_return_data['cels_log_return'].ix[i-2]\n",
    "  cels_log_return_3 = log_return_data['cels_log_return'].ix[i-3]\n",
    "  asx_cti_log_return_0 = log_return_data['asx_cti_log_return'].ix[i]\n",
    "  asx_cti_log_return_1 = log_return_data['asx_cti_log_return'].ix[i-1]\n",
    "  asx_cti_log_return_2 = log_return_data['asx_cti_log_return'].ix[i-2]\n",
    "  ftse_eo100_log_return_0 = log_return_data['ftse_eo100_log_return'].ix[i]\n",
    "  ftse_eo100_log_return_1 = log_return_data['ftse_eo100_log_return'].ix[i-1]\n",
    "  ftse_eo100_log_return_2 = log_return_data['ftse_eo100_log_return'].ix[i-2]  \n",
    "  dax_eusdn_log_return_0 = log_return_data['dax_eusdn_log_return'].ix[i]\n",
    "  dax_eusdn_log_return_1 = log_return_data['dax_eusdn_log_return'].ix[i-1]\n",
    "  dax_eusdn_log_return_2 = log_return_data['dax_eusdn_log_return'].ix[i-2] \n",
    "  n8wh_log_return_0 = log_return_data['n8wh_log_return'].ix[i]\n",
    "  n8wh_log_return_1 = log_return_data['n8wh_log_return'].ix[i-1]\n",
    "  n8wh_log_return_2 = log_return_data['n8wh_log_return'].ix[i-2] \n",
    "  dei_log_return_1 = log_return_data['dei_log_return'].ix[i-1]\n",
    "  dei_log_return_2 = log_return_data['dei_log_return'].ix[i-2]\n",
    "  dei_log_return_3 = log_return_data['dei_log_return'].ix[i-3]\n",
    "  positive_log_return_0 = log_return_data['positive_log_return'].ix[i]\n",
    "  positive_log_return_1 = log_return_data['positive_log_return'].ix[i-1]\n",
    "  positive_log_return_2 = log_return_data['positive_log_return'].ix[i-2] \n",
    "  negative_log_return_0 = log_return_data['negative_log_return'].ix[i]\n",
    "  negative_log_return_1 = log_return_data['negative_log_return'].ix[i-1]\n",
    "  negative_log_return_2 = log_return_data['negative_log_return'].ix[i-2]   \n",
    "  training_test_data = training_test_data.append(\n",
    "    {'qcln_log_return_positive':qcln_log_return_positive,\n",
    "    'qcln_log_return_negative':qcln_log_return_negative,\n",
    "    'qcln_log_return_1':qcln_log_return_1,\n",
    "    'qcln_log_return_2':qcln_log_return_2,\n",
    "    'qcln_log_return_3':qcln_log_return_3,\n",
    "    'icln_log_return_1':icln_log_return_1,\n",
    "    'icln_log_return_2':icln_log_return_2,\n",
    "    'icln_log_return_3':icln_log_return_3,\n",
    "    'sp_gtced_log_return_1':sp_gtced_log_return_1,\n",
    "    'sp_gtced_log_return_2':sp_gtced_log_return_2,\n",
    "    'sp_gtced_log_return_3':sp_gtced_log_return_3,\n",
    "    'cels_log_return_1':cels_log_return_1,\n",
    "    'cels_log_return_2':cels_log_return_2,\n",
    "    'cels_log_return_3':cels_log_return_3,\n",
    "    'asx_cti_log_return_0':asx_cti_log_return_0,\n",
    "    'asx_cti_log_return_1':asx_cti_log_return_1,\n",
    "    'asx_cti_log_return_2':asx_cti_log_return_2,\n",
    "    'ftse_eo100_log_return_0':ftse_eo100_log_return_0,\n",
    "    'ftse_eo100_log_return_1':ftse_eo100_log_return_1,\n",
    "    'ftse_eo100_log_return_2':ftse_eo100_log_return_2,\n",
    "    'dax_eusdn_log_return_0':dax_eusdn_log_return_0,\n",
    "    'dax_eusdn_log_return_1':dax_eusdn_log_return_1,\n",
    "    'dax_eusdn_log_return_2':dax_eusdn_log_return_2,\n",
    "    'n8wh_log_return_0':n8wh_log_return_0,\n",
    "    'n8wh_log_return_1':n8wh_log_return_1,\n",
    "    'n8wh_log_return_2':n8wh_log_return_2,\n",
    "    'dei_log_return_1':dei_log_return_1,\n",
    "    'dei_log_return_2':dei_log_return_2,\n",
    "    'dei_log_return_3':dei_log_return_3,\n",
    "    'positive_log_return_0':positive_log_return_0,\n",
    "    'positive_log_return_1':positive_log_return_1,\n",
    "    'positive_log_return_2':positive_log_return_2,\n",
    "    'negative_log_return_0':negative_log_return_0,\n",
    "    'negative_log_return_1':negative_log_return_1,\n",
    "    'negative_log_return_2':negative_log_return_2},\n",
    "    ignore_index=True)\n",
    "  \n",
    "# training_test_data.describe()\n",
    "training_test_data.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, create the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "predictors_tf = training_test_data[training_test_data.columns[2:]]\n",
    "\n",
    "classes_tf = training_test_data[training_test_data.columns[:2]]\n",
    "\n",
    "training_set_size = int(len(training_test_data) * 0.8)\n",
    "test_set_size = len(training_test_data) - training_set_size\n",
    "\n",
    "training_predictors_tf = predictors_tf[:training_set_size]\n",
    "training_classes_tf = classes_tf[:training_set_size]\n",
    "test_predictors_tf = predictors_tf[training_set_size:]\n",
    "test_classes_tf = classes_tf[training_set_size:]\n",
    "\n",
    "# training_predictors_tf.describe()\n",
    "training_predictors_tf.head(3)\n",
    "# training_classes_tf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Define some metrics here to evaluate the models.\n",
    "\n",
    "* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall#Precision) -  The ability of the classifier not to label as positive a sample that is negative.\n",
    "* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall#Recall) - The ability of the classifier to find all the positive samples.\n",
    "* [F1 Score](https://en.wikipedia.org/wiki/F1_score) - A weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
    "* Accuracy - The percentage correctly predicted in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def tf_confusion_metrics(model, actual_classes, session, feed_dict):\n",
    "  predictions = tf.argmax(model, 1)\n",
    "  actuals = tf.argmax(actual_classes, 1)\n",
    "\n",
    "  ones_like_actuals = tf.ones_like(actuals)\n",
    "  zeros_like_actuals = tf.zeros_like(actuals)\n",
    "  ones_like_predictions = tf.ones_like(predictions)\n",
    "  zeros_like_predictions = tf.zeros_like(predictions)\n",
    "\n",
    "  tp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fp_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, zeros_like_actuals), \n",
    "        tf.equal(predictions, ones_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  fn_op = tf.reduce_sum(\n",
    "    tf.cast(\n",
    "      tf.logical_and(\n",
    "        tf.equal(actuals, ones_like_actuals), \n",
    "        tf.equal(predictions, zeros_like_predictions)\n",
    "      ), \n",
    "      \"float\"\n",
    "    )\n",
    "  )\n",
    "\n",
    "  tp, tn, fp, fn = \\\n",
    "    session.run(\n",
    "      [tp_op, tn_op, fp_op, fn_op], \n",
    "      feed_dict\n",
    "    )\n",
    "\n",
    "  tpr = float(tp)/(float(tp) + float(fn))\n",
    "  fpr = float(fp)/(float(tp) + float(fn))\n",
    "\n",
    "  accuracy = (float(tp) + float(tn))/(float(tp) + float(fp) + float(fn) + float(tn))\n",
    "\n",
    "  recall = tpr\n",
    "  precision = float(tp)/(float(tp) + float(fp))\n",
    "  \n",
    "  f1_score = (2 * (precision * recall)) / (precision + recall)\n",
    "  \n",
    "  print 'Precision = ', precision\n",
    "  print 'Recall = ', recall\n",
    "  print 'F1 Score = ', f1_score\n",
    "  print 'Accuracy = ', accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Binary classification with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, get some tensors flowing. The model is binary classification expressed in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"binary-classification.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# sess = tf.Session()\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "\n",
    "  # Define variables for the number of predictors and number of classes \n",
    "  num_predictors = len(training_predictors_tf.columns) # 24 in the default case\n",
    "  num_classes = len(training_classes_tf.columns) # 2 in the default case\n",
    "\n",
    "# Define placeholders for the data we feed into the process - feature data and actual classes.\n",
    "feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "actual_classes = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights = tf.Variable(tf.truncated_normal([num_predictors, num_classes], stddev=0.0001))\n",
    "biases = tf.Variable(tf.ones([num_classes]))\n",
    "\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our feature data and weights.\n",
    "model = tf.nn.softmax(tf.matmul(feature_data, weights) + biases)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "# Define a training step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "training_step = tf.train.AdamOptimizer(learning_rate=0.0002).minimize(cost)   #0.0001\n",
    "\n",
    "# sess = tf.Session()\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "# init = tf.initialize_all_variables()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll train the model over 30,000 iterations using the full dataset each time. Every thousandth iteration we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "def run_it():\n",
    "    for i in range(1, 30001):\n",
    "      sess.run(\n",
    "        training_step, \n",
    "        feed_dict={\n",
    "          feature_data: training_predictors_tf.values, \n",
    "          actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "        }\n",
    "      )\n",
    "      if i%5000 == 0:\n",
    "        print i, sess.run(\n",
    "          accuracy,\n",
    "          feed_dict={\n",
    "            feature_data: training_predictors_tf.values, \n",
    "            actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "          }\n",
    "        )\n",
    "\n",
    "start = time.time()       \n",
    "run_it()\n",
    "end = time.time()\n",
    "print 'Training Time = ', (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Accuracy on the training set is ~69%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feed_dict= {\n",
    "  feature_data: test_predictors_tf.values,\n",
    "  actual_classes: test_classes_tf.values.reshape(len(test_classes_tf.values), 2)\n",
    "}\n",
    "\n",
    "tf_confusion_metrics(model, actual_classes, sess, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Accuracy on the test set is ~60%.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Feed-forward neural network with two hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll now build a proper feed-forward neural net with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"feed-forward.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess1 = tf.Session()\n",
    "\n",
    "nl1 = 50\n",
    "nl2 = 25\n",
    "\n",
    "# Define variables for the number of predictors and number of classes \n",
    "num_predictors = len(training_predictors_tf.columns) # 33 \n",
    "num_classes = len(training_classes_tf.columns) # 2 \n",
    "\n",
    "# Define placeholders for the data we feed into the process - feature data and actual classes.\n",
    "feature_data = tf.placeholder(\"float\", [None, num_predictors])\n",
    "actual_classes = tf.placeholder(\"float\", [None, 2])\n",
    "\n",
    "# Define a matrix of weights and initialize it with some small random values.\n",
    "weights1 = tf.Variable(tf.truncated_normal([num_predictors,nl1], stddev=0.0001))  #33, 50\n",
    "biases1 = tf.Variable(tf.ones([nl1]))  # 50\n",
    "\n",
    "weights2 = tf.Variable(tf.truncated_normal([nl1, nl2], stddev=0.0001))  # 50, 25\n",
    "biases2 = tf.Variable(tf.ones([nl2]))  # 25\n",
    "                     \n",
    "weights3 = tf.Variable(tf.truncated_normal([nl2, 2], stddev=0.0001))   # 25, 2\n",
    "biases3 = tf.Variable(tf.ones([2]))   # 2\n",
    "\n",
    "hidden_layer_1 = tf.nn.relu(tf.matmul(feature_data, weights1) + biases1)   # rectified linear unit (ReLU).\n",
    "hidden_layer_2 = tf.nn.relu(tf.matmul(hidden_layer_1, weights2) + biases2)\n",
    "\n",
    "# Define our model...\n",
    "# Here we take a softmax regression of the product of our feature data and weights.\n",
    "model = tf.nn.softmax(tf.matmul(hidden_layer_2, weights3) + biases3)\n",
    "\n",
    "# Define a cost function (we're using the cross entropy).\n",
    "cost = -tf.reduce_sum(actual_classes*tf.log(model))\n",
    "\n",
    "# Define a training step...\n",
    "# Here we use gradient descent with a learning rate of 0.01 using the cost function we just defined.\n",
    "train_op1 = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n",
    "\n",
    "# init = tf.initialize_all_variables()\n",
    "init = tf.global_variables_initializer()\n",
    "sess1.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We'll train the model over 15,000 iterations using the full dataset each time. Every thousandth iteration, we'll assess the accuracy of the model on the training data to assess progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(actual_classes, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "for i in range(1, 15001):  \n",
    "  sess1.run(\n",
    "    train_op1, \n",
    "    feed_dict={\n",
    "      feature_data: training_predictors_tf.values, \n",
    "      actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "    }\n",
    "  )\n",
    "  if i%5000 == 0:\n",
    "    print i, sess1.run(\n",
    "      accuracy,\n",
    "      feed_dict={\n",
    "        feature_data: training_predictors_tf.values, \n",
    "        actual_classes: training_classes_tf.values.reshape(len(training_classes_tf.values), 2)\n",
    "      }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "A significant improvement in accuracy with the training data shows that the hidden layers are adding additional capacity for learning to the model.\n",
    "Looking at precision, recall, and accuracy, we can see improvement in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feed_dict= {\n",
    "  feature_data: test_predictors_tf.values,\n",
    "  actual_classes: test_classes_tf.values.reshape(len(test_classes_tf.values), 2)\n",
    "}\n",
    "\n",
    "tf_confusion_metrics(model, actual_classes, sess1, feed_dict)\n",
    "sess1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"NN-Results.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "As anticipated, there is a clear dependency between the data and the Neural Network model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Over ~75% accuracy in predicting the close of the Nasdaq Clean Energy Index is the highest we've seen achieved on this dataset. The reason for the relatively modest accuracy achieved is the dataset itself; there isn't enough signal in the current dataset but ~7 times out of 10, we were able to correctly determine if the Nasdaq Clean Energy Index would close up or down on the day, and that's objectively good."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
