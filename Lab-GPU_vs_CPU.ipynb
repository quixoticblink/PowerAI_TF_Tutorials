{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### List of CPU and GPUs\n",
    "How to get list of CPU and GPUs ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos]\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "What is __XLA__?\n",
    "XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that optimizes TensorFlow computations. The results are improvements in speed, memory usage, and portability on server and mobile platforms. Initially, most users will not see large benefits from XLA, but are welcome to experiment by using XLA via just-in-time (JIT) compilation or ahead-of-time (AOT) compilation. Developers targeting new hardware accelerators are especially encouraged to try out XLA.\n",
    "\n",
    "The XLA framework is experimental and in active development. In particular, while it is unlikely that the semantics of existing operations will change, it is expected that more operations will be added to cover important use cases. The team welcomes feedback from the community about missing functionality and community contributions via GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "I also recommend logging device placement when using GPUs, at this lets you easily debug issues relating to different device usage. This prints the usage of devices to the log, allowing you to see when devices change and how that affects the graph.\n",
    "\n",
    "You can see that a, b and c are all run on GPU0\n",
    "\n",
    "### ***Run it in a terminal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_logging_device():\n",
    "    # Creates a graph.\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "    # Creates a session with log_device_placement set to True.\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    # Runs the op.\n",
    "    print sess.run(c)\n",
    "print_logging_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication on gpu0 and cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print \"------- Multiplication on gpu0 vs cpu ---------\"\n",
    "def matrix_mul(device_name, matrix_sizes):\n",
    "    time_values = []\n",
    "    #device_name = \"/cpu:0\"\n",
    "    for size in matrix_sizes:\n",
    "        with tf.device(device_name):\n",
    "            random_matrix = tf.random_uniform(shape=(2,2), minval=0, maxval=1)\n",
    "            dot_operation = tf.matmul(random_matrix, tf.transpose(random_matrix))\n",
    "            sum_operation = tf.reduce_sum(dot_operation)\n",
    "\n",
    "        with tf.Session(config=tf.ConfigProto(log_device_placement=False)) as session:\n",
    "            startTime = datetime.now()\n",
    "            result = session.run(sum_operation)\n",
    "        td = datetime.now() - startTime\n",
    "        time_values.append(td.microseconds/1000)\n",
    "        print (\"matrix shape:\" + str(size) + \"  --\"+ device_name +\" time: \"+str(td.microseconds/1000))\n",
    "    return time_values\n",
    "\n",
    "\n",
    "matrix_sizes = range(100,1000,100)\n",
    "time_values_gpu = matrix_mul(\"/gpu:0\", matrix_sizes)\n",
    "time_values_cpu = matrix_mul(\"/cpu:0\", matrix_sizes)\n",
    "print (\"GPU time\" +  str(time_values_gpu))\n",
    "print (\"CPUtime\" + str(time_values_cpu))\n",
    "print \"--------------------------------\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(matrix_sizes[:len(time_values_gpu)], time_values_gpu, label='cpu')\n",
    "plt.plot(matrix_sizes[:len(time_values_cpu)], time_values_cpu, label='gpu')\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.xlabel('Size of Matrix ')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Using Multi GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print \"------- Multi GPU ---------\"\n",
    "def multi_gpu():\n",
    "    c = []\n",
    "    for d in ['/cpu','/gpu:0', '/gpu:1','/gpu:2', '/gpu:3']:\n",
    "        with tf.device(d):\n",
    "            a = tf.ones(shape=[3,3], dtype=tf.float32)\n",
    "            b = tf.ones(shape=[3,3], dtype=tf.float32)\n",
    "            c.append(tf.matmul(a, b))\n",
    "    with tf.device('/cpu:0'):\n",
    "      sum = tf.add_n(c)\n",
    "    sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "    print(sess.run(sum))\n",
    "    \n",
    "multi_gpu()\n",
    "print \"--------------------------------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single vs Multi GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Processing Units logs\n",
    "log_device_placement = True\n",
    "\n",
    "#num of multiplications to perform\n",
    "n = 10\n",
    "\n",
    "matrix_size = 1e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: compute A^n + B^n on 2 GPUs\n",
    "\n",
    "# Create random large matrix\n",
    "A = np.random.rand(matrix_size, matrix_size).astype('float32')\n",
    "B = np.random.rand(matrix_size, matrix_size).astype('float32')\n",
    "C = np.random.rand(matrix_size, matrix_size).astype('float32')\n",
    "D = np.random.rand(matrix_size, matrix_size).astype('float32')\n",
    "\n",
    "# Creates a graph to store results\n",
    "c1 = []\n",
    "c2 = []\n",
    "\n",
    "# Define matrix power\n",
    "def matpow(M, n):\n",
    "    if n < 1: #Abstract cases where n < 1\n",
    "        return M\n",
    "    else:\n",
    "        return tf.matmul(M, matpow(M, n-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Single GPU computing\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant(A)\n",
    "    b = tf.constant(B)\n",
    "    c = tf.constant(C)\n",
    "    d = tf.constant(D)\n",
    "    #compute A^n, B^n, .. and store results in c1\n",
    "    c1.append(matpow(a, n))\n",
    "    c1.append(matpow(b, n))\n",
    "    c1.append(matpow(c, n))\n",
    "    c1.append(matpow(d, n))\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    sum = tf.add_n(c1) #Addition of all elements in c1, i.e. A^n + B^n + ..\n",
    "\n",
    "t1_1 = datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n",
    "    # Runs the op.\n",
    "    sess.run(sum)\n",
    "t2_1 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multi GPU computing\n",
    "# GPU:0 computes A^n\n",
    "with tf.device('/gpu:0'):\n",
    "    #compute A^n and store result in c2\n",
    "    a = tf.constant(A)\n",
    "    c2.append(matpow(a, n))\n",
    "\n",
    "#GPU:1 computes B^n\n",
    "with tf.device('/gpu:1'):\n",
    "    #compute B^n and store result in c2\n",
    "    b = tf.constant(B)\n",
    "    c2.append(matpow(b, n))\n",
    "\n",
    "\n",
    "#GPU:1 computes C^n\n",
    "with tf.device('/gpu:2'):\n",
    "    #compute B^n and store result in c2\n",
    "    c = tf.constant(C)\n",
    "    c2.append(matpow(c, n))    \n",
    "    \n",
    "    \n",
    "#GPU:1 computes D^n\n",
    "with tf.device('/gpu:3'):\n",
    "    #compute B^n and store result in c2\n",
    "    d = tf.constant(D)\n",
    "    c2.append(matpow(d, n))\n",
    "    \n",
    "    \n",
    "    \n",
    "with tf.device('/cpu:0'):\n",
    "    sum = tf.add_n(c2) #Addition of all elements in c2, i.e. A^n + B^n\n",
    "\n",
    "t1_2 = datetime.now()\n",
    "with tf.Session(config=tf.ConfigProto(log_device_placement=log_device_placement)) as sess:\n",
    "    # Runs the op.\n",
    "    sess.run(sum)\n",
    "t2_2 = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Single GPU computation time: \" + str(t2_1-t1_1)\n",
    "print \"Multi GPU computation time: \" + str(t2_2-t1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: Based on https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
